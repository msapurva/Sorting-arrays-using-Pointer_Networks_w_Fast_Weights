{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fast_Weights_based Pointer_net.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msapurva/Sorting-arrays-using-Pointer_Networks_w_Fast_Weights/blob/master/Fast_Weights_based_Pointer_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zWFaAnmetOTr",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime as dt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lLD0tC82or8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.style.use('ggplot')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-zyK1oRMrp0l",
        "colab": {}
      },
      "source": [
        "tf.enable_eager_execution()\n",
        "np.random.seed(42)\n",
        "tf.random.set_random_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KH5gPEjcrp0n"
      },
      "source": [
        "### Experiment Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sWdDXmrXrp0n",
        "colab": {}
      },
      "source": [
        "# Training and Dev Data\n",
        "minSeqSize=2\n",
        "maxSeqSize=5\n",
        "\n",
        "# Training Data \n",
        "batchSize=32\n",
        "numOfBatches=200\n",
        "datasize=batchSize*numOfBatches*(maxSeqSize-minSeqSize+1)\n",
        "\n",
        "# Dev data\n",
        "devBatchSize=64\n",
        "devNumBatches=10\n",
        "\n",
        "# Test data\n",
        "testBatchSize=64\n",
        "testNumBatches=10\n",
        "\n",
        "\n",
        "# Ptr Model Config\n",
        "hidden_dimensions=200\n",
        "\n",
        "# FW Model Config(directly modifies the class default values)\n",
        "fwS=1\n",
        "fwlearningrate=0.5\n",
        "fwdecayrate=0.9\n",
        "\n",
        "# For reproducing ablation on input dimension on transformation.\n",
        "# A None value leads the system to use the default value of hidden_dimensions. \n",
        "# Change this to required integer to get a new value\n",
        "given_fw_x_size=None\n",
        "\n",
        "\n",
        "\n",
        "# over-fitting tolerance cut-off\n",
        "# for some cases the initial batch or an intermediate batch might cross this due to random chance and not due to overfitting. \n",
        "#In such cases modify this and run the code again\n",
        "overfitcutoff=0.1 \n",
        "\n",
        "# epoch configuration defined later below. Look at epochsToRun variable to tune this"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wmYMjSIyL2F8"
      },
      "source": [
        "### Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xbja4GErPikD",
        "colab": {}
      },
      "source": [
        "def makeData(minSeqSize,maxSeqSize,batchSize,numOfBatches):\n",
        "    X={}\n",
        "    Y={}\n",
        "    datasize=batchSize*numOfBatches*(maxSeqSize-minSeqSize+1)\n",
        "\n",
        "    for seqLen in range(minSeqSize,maxSeqSize+1):\n",
        "        X[seqLen]=[]\n",
        "        Y[seqLen]=[]\n",
        "\n",
        "        for dataidx in range(int(datasize/(maxSeqSize-minSeqSize+1))):\n",
        "            seqBase=np.random.uniform(size=(seqLen))\n",
        "            aSeq=seqBase\n",
        "            X[seqLen]+=[aSeq]\n",
        "            aRec=[]\n",
        "            for e in np.sort(seqBase):\n",
        "                idx=list(seqBase).index(e)\n",
        "                aRec+=[np.zeros(seqLen,dtype=np.float32)]\n",
        "                aRec[-1][idx]=1\n",
        "            Y[seqLen]+=[aRec]\n",
        "\n",
        "        X[seqLen]=np.array(X[seqLen],dtype=np.float32)\n",
        "        X[seqLen]=np.reshape(X[seqLen],[X[seqLen].shape[0],X[seqLen].shape[1],1])\n",
        "        Y[seqLen]=np.array(Y[seqLen],dtype=np.float32)\n",
        "    return X,Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s6ZambmvF72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainX,trainY=makeData(minSeqSize,maxSeqSize,batchSize,numOfBatches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dsYIS0Terp0t",
        "outputId": "43e75c88-5a44-45a3-be24-f2e4edba73db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Train X for',minSeqSize,':',trainX[minSeqSize].shape)\n",
        "print('Train Y for',minSeqSize,':',trainY[minSeqSize].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train X for 2 : (6400, 2, 1)\n",
            "Train Y for 2 : (6400, 2, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "soZhrxTx6-Uy"
      },
      "source": [
        "### Pointer Network with Fast Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YkcI--KctfRn",
        "colab": {}
      },
      "source": [
        "class FastWeightsCell(tf.keras.Model):\n",
        "    def __init__(self,features=1, output_size=1,decay_rate = fwdecayrate, learning_rate = fwlearningrate, hidden_size=50):\n",
        "        super(FastWeightsCell, self).__init__()\n",
        "        self.DR = decay_rate \n",
        "        self.LR = learning_rate \n",
        "        self.features=features\n",
        "        self.output_size=output_size\n",
        "        self.hidden_size = hidden_size \n",
        "\n",
        "        \n",
        "        self.W_x = tf.Variable(tf.random_uniform([self.hidden_size,self.features], -np.sqrt(2/self.features), np.sqrt(2/self.features)), dtype=tf.float32)\n",
        "        self.B_x = tf.Variable(tf.zeros([self.hidden_size,1]), dtype=tf.float32)\n",
        "        self.W_h = tf.Variable(initial_value = 0.5 * np.identity(self.hidden_size), dtype = tf.float32)\n",
        "        self.W_y = tf.Variable(tf.random_uniform([self.output_size,self.hidden_size], -np.sqrt(2/self.hidden_size), np.sqrt(2/self.hidden_size)), dtype = tf.float32)\n",
        "        self.B_y = tf.Variable(tf.zeros([self.output_size,1]), dtype= tf.float32)\n",
        "        self.scale = tf.Variable(tf.ones([1,self.hidden_size,1]), dtype = tf.float32) \n",
        "        self.shift = tf.Variable(tf.zeros([1,self.hidden_size,1]), dtype = tf.float32)\n",
        "        \n",
        "        # values of A and H matricies\n",
        "        self.A = None  \n",
        "        self.H = None  \n",
        "\n",
        "    def call(self,X,H,A,S=fwS):\n",
        "        X = tf.cast(X, tf.float32)\n",
        "        self.H=H\n",
        "        self.A=A\n",
        "        self.A = tf.scalar_mul(self.DR, self.A) + tf.scalar_mul(self.LR,(tf.matmul(H, tf.transpose(H,perm=[0,2,1]))))\n",
        "        broad_W_h=tf.broadcast_to(self.W_h,[self.H.shape[0].value,self.W_h.shape[0].value,self.W_h.shape[1].value])\n",
        "        broad_W_x=tf.broadcast_to(self.W_x,[self.H.shape[0].value,self.W_x.shape[0].value,self.W_x.shape[1].value])\n",
        "        H_s=tf.nn.relu(tf.matmul(broad_W_h,self.H)+tf.matmul(broad_W_x,X)        +     tf.expand_dims(self.B_x,0))\n",
        "        for _ in range(S):\n",
        "            H_s=         tf.matmul(self.A,H_s)  +    tf.matmul(broad_W_h,self.H) + tf.matmul(broad_W_x,X)  +  tf.expand_dims(self.B_x,0)                    \n",
        "            # applying Layer Normalization \n",
        "            mean, var = tf.nn.moments(H_s, axes =0, keep_dims = True)\n",
        "            H_s = (self.scale*(H_s - mean))/(tf.sqrt(var + 1e-5) + self.shift)           \n",
        "            # applying non linearity\n",
        "            H_s = tf.nn.relu(H_s)        \n",
        "        self.H = H_s\n",
        "        broad_W_y=tf.broadcast_to(self.W_y,[self.H.shape[0].value,self.W_y.shape[0].value,self.W_y.shape[1].value])\n",
        "        output = tf.matmul(broad_W_y,self.H) + tf.expand_dims(self.B_y,0)\n",
        "        # --- return output, hidden state, and value of matrix A ---\n",
        "        return output, self.H, self.A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UpDh6QO-7d4a",
        "colab": {}
      },
      "source": [
        "class FWPointerNetwork(tf.keras.Model):\n",
        "    def __init__(self,features=1, output_size=None, hidden_size=50, fw_x_size=None, attentionInternalDim=None):\n",
        "        super(FWPointerNetwork, self).__init__()\n",
        "        self.features = features\n",
        "        self.output_size=output_size\n",
        "        if self.output_size is None:\n",
        "            self.output_size=features\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attentionInternalDim=attentionInternalDim\n",
        "        if self.attentionInternalDim is None:\n",
        "            self.attentionInternalDim=self.hidden_size\n",
        "        self.fw_x_size=fw_x_size\n",
        "        if self.fw_x_size is None:\n",
        "            self.fw_x_size=self.hidden_size\n",
        "\n",
        "        # Encoder and Encoder Input\n",
        "        self.encodercell = FastWeightsCell(features=self.fw_x_size, output_size=self.output_size,hidden_size=self.hidden_size)\n",
        "        self.W_e = tf.Variable(tf.random_uniform([self.fw_x_size, self.features], -0.08, 0.08), dtype=tf.float32)    \n",
        "        self.B_e = tf.Variable(tf.random_uniform([self.fw_x_size, 1], -0.08, 0.08), dtype=tf.float32)\n",
        "\n",
        "        # Decoder and Decoder Input\n",
        "        self.decodercell = FastWeightsCell(features=self.fw_x_size, output_size=self.output_size,hidden_size=self.hidden_size)\n",
        "        self.W_d = tf.Variable(tf.random_uniform([self.fw_x_size, self.features], -0.08, 0.08), dtype=tf.float32)    \n",
        "        self.B_d = tf.Variable(tf.random_uniform([self.fw_x_size, 1], -0.08, 0.08), dtype=tf.float32)\n",
        "\n",
        "        # Attention Related Vector with bias\n",
        "        self.W1 = tf.Variable(tf.random_uniform([self.attentionInternalDim,self.hidden_size], -0.08, 0.08), dtype=tf.float32)\n",
        "        self.W2 = tf.Variable(tf.random_uniform([self.attentionInternalDim,self.hidden_size], -0.08, 0.08), dtype=tf.float32)\n",
        "        self.V = tf.Variable(tf.random_uniform([self.attentionInternalDim, 1], -0.5, 0.5), dtype=tf.float32)\n",
        "        self.B_ptr = tf.Variable(tf.random_uniform([self.attentionInternalDim,1], -0.08, 0.08), dtype=tf.float32)\n",
        "\n",
        "\n",
        "    def call(self, X):\n",
        "        givenbatchsize=X.shape[0].value\n",
        "        givenSeqLen=X.shape[1].value\n",
        "        eHstates = []\n",
        "        eH = tf.zeros([givenbatchsize, self.hidden_size,1],dtype=np.float32)\n",
        "        eA = tf.zeros([givenbatchsize, self.hidden_size,self.hidden_size],dtype=np.float32)\n",
        "        for i in range(givenSeqLen):            \n",
        "            X_i = tf.transpose(X[:,i:i+1],[0,2,1])\n",
        "            broad_W_e=tf.broadcast_to(self.W_e,[givenbatchsize,self.W_e.shape[0].value,self.W_e.shape[1].value])\n",
        "            cell_input = tf.nn.elu(tf.matmul(broad_W_e,X_i) + tf.expand_dims(self.B_e,0))\n",
        "            _, eH,        eA        = self.encodercell(cell_input, eH, eA)\n",
        "            eHstates.append(eH)\n",
        "\n",
        "        dH = eHstates[-1]\n",
        "        dA = tf.zeros([givenbatchsize, self.hidden_size,self.hidden_size],dtype=np.float32)\n",
        "        X_i = tf.constant(-1,dtype=np.float32,shape=[givenbatchsize,self.features,1])\n",
        "        attentionVector_all=None\n",
        "        outputVector_all=None\n",
        "        identity=tf.eye(givenSeqLen,dtype=np.float32)\n",
        "        \n",
        "        for i in range(givenSeqLen):\n",
        "            broad_W_d=tf.broadcast_to(self.W_d,[givenbatchsize,self.W_d.shape[0].value,self.W_d.shape[1].value])\n",
        "            cell_input = tf.nn.elu(tf.matmul(broad_W_d,X_i) + tf.expand_dims(self.B_d,0))\n",
        "            _, dH,        dA        = self.decodercell(cell_input, dH, dA)\n",
        "            broad_W1 = tf.broadcast_to(self.W1,[givenbatchsize,self.W1.shape[0].value,self.W1.shape[1].value])            \n",
        "            broad_W2 = tf.broadcast_to(self.W2,[givenbatchsize,self.W2.shape[0].value,self.W2.shape[1].value])\n",
        "            broad_V = tf.broadcast_to(self.V,[givenbatchsize,self.V.shape[0].value,self.V.shape[1].value])\n",
        "            d_part=tf.matmul(broad_W2,dH)            \n",
        "            attentionVector_i=None\n",
        "            for encoderIndex in range(givenSeqLen):\n",
        "                attentionVector_component=tf.matmul(broad_W1,eHstates[encoderIndex])+ d_part     + tf.expand_dims(self.B_ptr,0)\n",
        "                attentionVector_component=tf.nn.elu(attentionVector_component)\n",
        "                attentionVector_component=tf.matmul(tf.transpose(broad_V,perm=[0,2,1]),attentionVector_component)\n",
        "                if attentionVector_i is None:\n",
        "                    attentionVector_i=attentionVector_component\n",
        "                else:\n",
        "                    attentionVector_i=tf.concat([attentionVector_i,attentionVector_component],axis=2)\n",
        "            attentionVector_i=tf.nn.softmax(attentionVector_i,axis=2)            \n",
        "            if attentionVector_all is None:\n",
        "                attentionVector_all=attentionVector_i\n",
        "            else:\n",
        "                attentionVector_all=tf.concat(       [attentionVector_all  ,   attentionVector_i   ],axis=1)            \n",
        "            predictedOutput_i  =   tf.cast(tf.argmax(attentionVector_i,axis=2),dtype=np.int32)\n",
        "            extractionMatrix=tf.nn.embedding_lookup(identity,tf.reshape(predictedOutput_i,[predictedOutput_i.shape[0]]))\n",
        "            extractionMatrix=tf.expand_dims(extractionMatrix,axis=1)\n",
        "            X_i=tf.matmul(  tf.cast(extractionMatrix,dtype=np.float32) ,      X   )\n",
        "            if outputVector_all is None:\n",
        "                outputVector_all=X_i\n",
        "            else:\n",
        "                outputVector_all=tf.concat([outputVector_all,X_i],axis=1)\n",
        "            X_i=tf.transpose(X_i,[0,2,1])\n",
        "        return attentionVector_all,outputVector_all "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoPzn01j48ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def runAndGetLoss(X,Y,model,isVerbose=0,headlimit=-1):\n",
        "\n",
        "    attn,output=model(X)\n",
        "    loss=tf.reduce_mean(tf.keras.backend.categorical_crossentropy(Y, attn))\n",
        "    loss=(loss/float(maxSeqSize-minSeqSize+1))\n",
        "    \n",
        "    if isVerbose>0:\n",
        "      for instance_idx in range(X.shape[0]):\n",
        "        print(\"====== Actual :\",np.reshape(X[instance_idx],X[instance_idx].shape[0]))\n",
        "        print('  - Predicted :',np.reshape(output[instance_idx],output[instance_idx].shape[0]))\n",
        "        print('\\n\\n')\n",
        "        if headlimit!=-1 and headlimit<instance_idx:\n",
        "          break    \n",
        "    return attn,output,loss\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9qaIyvevF8c",
        "colab_type": "text"
      },
      "source": [
        "### Instantiate the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYHI-KQKvF8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fwpn = FWPointerNetwork(features=trainX[minSeqSize][0].shape[1], output_size=trainX[minSeqSize][0].shape[1], hidden_size=hidden_dimensions,fw_x_size=given_fw_x_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LkM2Lp_zq3VK"
      },
      "source": [
        "### Output of Network Before Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dum0ZXWSH9z2",
        "colab_type": "code",
        "outputId": "b60ec616-2b26-48f1-e9b9-3708c26bfd24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4474
        }
      },
      "source": [
        "# Use data point\n",
        "seqLen=minSeqSize\n",
        "attn,output,aloss=runAndGetLoss(trainX[seqLen][0:batchSize],trainY[seqLen][0:batchSize],fwpn,1)\n",
        "print(\"loss:\",aloss)\n",
        "print(\"attn:\")\n",
        "print(attn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:642: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "====== Actual : [0.37454012 0.9507143 ]\n",
            "  - Predicted : [0.37454012 0.37454012]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.7319939 0.5986585]\n",
            "  - Predicted : [0.7319939 0.7319939]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.15601864 0.15599452]\n",
            "  - Predicted : [0.15601864 0.15601864]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.05808361 0.8661761 ]\n",
            "  - Predicted : [0.8661761 0.8661761]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.601115  0.7080726]\n",
            "  - Predicted : [0.601115 0.601115]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.02058449 0.96990985]\n",
            "  - Predicted : [0.96990985 0.96990985]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.83244264 0.21233912]\n",
            "  - Predicted : [0.21233912 0.21233912]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.18182497 0.1834045 ]\n",
            "  - Predicted : [0.18182497 0.18182497]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.30424225 0.52475643]\n",
            "  - Predicted : [0.30424225 0.30424225]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.43194503 0.29122913]\n",
            "  - Predicted : [0.43194503 0.43194503]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.6118529  0.13949387]\n",
            "  - Predicted : [0.6118529 0.6118529]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.29214466 0.36636186]\n",
            "  - Predicted : [0.29214466 0.29214466]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.45606998 0.785176  ]\n",
            "  - Predicted : [0.45606998 0.45606998]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.19967379 0.5142344 ]\n",
            "  - Predicted : [0.19967379 0.19967379]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.59241456 0.04645041]\n",
            "  - Predicted : [0.59241456 0.59241456]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.60754484 0.17052412]\n",
            "  - Predicted : [0.60754484 0.60754484]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.06505159 0.94888556]\n",
            "  - Predicted : [0.94888556 0.94888556]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.965632   0.80839735]\n",
            "  - Predicted : [0.80839735 0.80839735]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.30461377 0.09767211]\n",
            "  - Predicted : [0.30461377 0.30461377]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.684233  0.4401525]\n",
            "  - Predicted : [0.684233 0.684233]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.12203824 0.4951769 ]\n",
            "  - Predicted : [0.12203824 0.12203824]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.03438852 0.9093204 ]\n",
            "  - Predicted : [0.9093204 0.9093204]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.25877997 0.66252226]\n",
            "  - Predicted : [0.25877997 0.25877997]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.31171107 0.52006805]\n",
            "  - Predicted : [0.31171107 0.31171107]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.54671025 0.18485446]\n",
            "  - Predicted : [0.54671025 0.54671025]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.96958464 0.77513283]\n",
            "  - Predicted : [0.77513283 0.77513283]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.93949896 0.89482737]\n",
            "  - Predicted : [0.89482737 0.89482737]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.5979    0.9218742]\n",
            "  - Predicted : [0.5979 0.5979]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.08849251 0.19598286]\n",
            "  - Predicted : [0.19598286 0.19598286]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.04522729 0.32533032]\n",
            "  - Predicted : [0.32533032 0.32533032]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.3886773  0.27134904]\n",
            "  - Predicted : [0.3886773 0.3886773]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.8287375  0.35675332]\n",
            "  - Predicted : [0.8287375 0.8287375]\n",
            "\n",
            "\n",
            "\n",
            "loss: tf.Tensor(0.21998727, shape=(), dtype=float32)\n",
            "attn:\n",
            "tf.Tensor(\n",
            "[[[0.6069081  0.3930919 ]\n",
            "  [0.6069081  0.3930919 ]]\n",
            "\n",
            " [[0.74301773 0.2569823 ]\n",
            "  [0.74301773 0.2569823 ]]\n",
            "\n",
            " [[0.7477492  0.25225085]\n",
            "  [0.7477492  0.25225085]]\n",
            "\n",
            " [[0.22579902 0.7742009 ]\n",
            "  [0.22457176 0.7754283 ]]\n",
            "\n",
            " [[0.71378887 0.28621113]\n",
            "  [0.71378887 0.28621113]]\n",
            "\n",
            " [[0.12322586 0.8767741 ]\n",
            "  [0.12450888 0.8754911 ]]\n",
            "\n",
            " [[0.4930251  0.5069749 ]\n",
            "  [0.4930251  0.5069749 ]]\n",
            "\n",
            " [[0.8390205  0.16097955]\n",
            "  [0.8390205  0.16097955]]\n",
            "\n",
            " [[0.74190634 0.25809368]\n",
            "  [0.74190634 0.25809368]]\n",
            "\n",
            " [[0.5141781  0.48582193]\n",
            "  [0.5141781  0.48582193]]\n",
            "\n",
            " [[0.7243199  0.2756801 ]\n",
            "  [0.7243199  0.2756801 ]]\n",
            "\n",
            " [[0.7610987  0.23890136]\n",
            "  [0.7610987  0.23890136]]\n",
            "\n",
            " [[0.54732096 0.452679  ]\n",
            "  [0.54732096 0.452679  ]]\n",
            "\n",
            " [[0.87073565 0.12926431]\n",
            "  [0.87073565 0.12926431]]\n",
            "\n",
            " [[0.70505255 0.29494745]\n",
            "  [0.70505255 0.29494745]]\n",
            "\n",
            " [[0.7201294  0.27987063]\n",
            "  [0.7201294  0.27987063]]\n",
            "\n",
            " [[0.26528966 0.73471034]\n",
            "  [0.2635597  0.7364403 ]]\n",
            "\n",
            " [[0.05504561 0.9449544 ]\n",
            "  [0.05506917 0.9449308 ]]\n",
            "\n",
            " [[0.74129856 0.2587014 ]\n",
            "  [0.74129856 0.2587014 ]]\n",
            "\n",
            " [[0.7869007  0.2130993 ]\n",
            "  [0.7869007  0.2130993 ]]\n",
            "\n",
            " [[0.5864915  0.4135085 ]\n",
            "  [0.5864915  0.4135085 ]]\n",
            "\n",
            " [[0.15415801 0.84584194]\n",
            "  [0.13185282 0.86814713]]\n",
            "\n",
            " [[0.8078632  0.19213687]\n",
            "  [0.8078632  0.19213687]]\n",
            "\n",
            " [[0.729452   0.270548  ]\n",
            "  [0.729452   0.270548  ]]\n",
            "\n",
            " [[0.65625024 0.3437497 ]\n",
            "  [0.65625024 0.3437497 ]]\n",
            "\n",
            " [[0.04934083 0.95065916]\n",
            "  [0.04967674 0.9503233 ]]\n",
            "\n",
            " [[0.12255151 0.87744856]\n",
            "  [0.1233516  0.87664837]]\n",
            "\n",
            " [[0.7105813  0.2894187 ]\n",
            "  [0.7105813  0.2894187 ]]\n",
            "\n",
            " [[0.36060593 0.63939404]\n",
            "  [0.3571047  0.64289534]]\n",
            "\n",
            " [[0.16433913 0.8356609 ]\n",
            "  [0.14534885 0.85465115]]\n",
            "\n",
            " [[0.57551366 0.42448634]\n",
            "  [0.57551366 0.42448634]]\n",
            "\n",
            " [[0.5004799  0.49952012]\n",
            "  [0.5004799  0.49952012]]], shape=(32, 2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcYXZivctf44",
        "colab_type": "text"
      },
      "source": [
        "### Dev Data Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aog5pAPYvF8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create dev data and batches\n",
        "devX,devY=makeData(minSeqSize,maxSeqSize,devBatchSize,devNumBatches)\n",
        "testX,testY=makeData(minSeqSize,maxSeqSize+1+5,testBatchSize,testNumBatches)\n",
        "def getTestLoss():\n",
        "  # print 1\n",
        "  aloss=0\n",
        "  aloss1=0\n",
        "  aloss2=0\n",
        "\n",
        "  print(\"SUMMARY\")\n",
        "  for seqLen in range(minSeqSize,maxSeqSize+1):\n",
        "    X,Y=makeData(seqLen,seqLen,testBatchSize,testNumBatches)\n",
        "    _,_,interdevLoss=runAndGetLoss(X[seqLen],Y[seqLen],fwpn)\n",
        "    aloss=interdevLoss\n",
        "    print('Seq:',seqLen,'Size:',X[seqLen].shape[0],'loss:',aloss.numpy())\n",
        "    aloss1+=aloss\n",
        "\n",
        "  print('---')\n",
        "\n",
        "  for seqLen in range(maxSeqSize+1,maxSeqSize+1+5):\n",
        "    X,Y=makeData(seqLen,seqLen,testBatchSize,testNumBatches)\n",
        "    _,_,interdevLoss=runAndGetLoss(X[seqLen],Y[seqLen],fwpn)\n",
        "    aloss=interdevLoss\n",
        "    print('Seq:',seqLen,'Size:',X[seqLen].shape[0],'loss:',aloss.numpy())\n",
        "    aloss2+=aloss\n",
        "\n",
        "  print('loss1:',np.round(aloss1.numpy()/float(maxSeqSize-minSeqSize+1),6))\n",
        "  print('loss2:',np.round(aloss2.numpy()/float(5),6))\n",
        "  return aloss1,aloss2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5YMQpvDvF8g",
        "colab_type": "text"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEJUt4aEvF8h",
        "colab_type": "text"
      },
      "source": [
        "##### Config Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZxbw-BKvF8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create batches of training data\n",
        "batchedDataset={}\n",
        "for seqLen in range(minSeqSize,maxSeqSize+1):\n",
        "    batchedDataset[seqLen]=[]\n",
        "    for aBatch in tf.data.Dataset.from_tensor_slices((trainX[seqLen],trainY[seqLen])).batch(batchSize):\n",
        "        batchedDataset[seqLen]+=[aBatch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48083hMdvF8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training configuration\n",
        "lastepoch=-1\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "loss_history = []\n",
        "wholeLoss_history=[]\n",
        "devLoss_history=[]\n",
        "total_attention = []\n",
        "bestwholeloss=10000000\n",
        "bestdevloss=10000000\n",
        "bestepoch=-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVDN0tMzvF8l",
        "colab_type": "text"
      },
      "source": [
        "##### Run Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNUEbgorvF8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochsToRun = 10\n",
        "overfitFlag=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "uzLakdiwvF8m",
        "colab_type": "code",
        "outputId": "04fe3270-e14f-44bc-fdf6-4e1a66bf49bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12648
        }
      },
      "source": [
        "starttime=dt.now()\n",
        "maxEpochsforthis=lastepoch+1+epochsToRun\n",
        "for epoch in range(lastepoch+1,maxEpochsforthis):\n",
        "    if overfitFlag:\n",
        "        break\n",
        "    lastepoch=epoch\n",
        "    print(\"New Epoch Started:\",epoch)\n",
        "    for batch in range(numOfBatches):\n",
        "        wholeLoss=0\n",
        "        for seqLen in range(minSeqSize,maxSeqSize+1):\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                input_data, target_data=batchedDataset[seqLen][batch]\n",
        "                _,_,batch_loss=runAndGetLoss(input_data,target_data,fwpn)\n",
        "            variables = fwpn.variables\n",
        "            grads = tape.gradient(batch_loss, variables)\n",
        "            optimizer.apply_gradients(zip(grads, variables), global_step=tf.train.get_or_create_global_step())\n",
        "            loss_history.append(batch_loss.numpy())\n",
        "            wholeLoss+=batch_loss/float(maxSeqSize-minSeqSize+1)\n",
        "        for seqLen in range(minSeqSize,maxSeqSize+1):\n",
        "            wholeLoss_history.append(wholeLoss.numpy())\n",
        "            \n",
        "        if batch % 10 == 0:\n",
        "            print(\"\\tEpoch {:03d}/{:03d}: Loss at batchSetNum {:02d}: {:.9f}\".format((epoch), maxEpochsforthis-1, batch, wholeLoss),np.round(((dt.now()-starttime).seconds)/60.0,1), \"mins\")\n",
        "\n",
        "            devLoss=0\n",
        "            \n",
        "            for seqLen in range(minSeqSize,maxSeqSize+1):\n",
        "                _,_,interdevLoss=runAndGetLoss(devX[seqLen],devY[seqLen],fwpn)\n",
        "                devLoss+=interdevLoss/float(maxSeqSize-minSeqSize+1)\n",
        "\n",
        "            for _ in range((maxSeqSize-minSeqSize+1)*10):\n",
        "              devLoss_history.append(devLoss.numpy())\n",
        "            \n",
        "            print('\\t               WholeLoss:',np.round(wholeLoss.numpy(),5),'DevLoss:',np.round(devLoss,5))\n",
        "            \n",
        "            if len(devLoss_history)==0:\n",
        "              bestdevloss=devLoss\n",
        "              bestwholeloss=wholeLoss\n",
        "              bestepoch=lastepoch\n",
        "              besttestloss1,besttestloss2=getTestLoss()\n",
        "            \n",
        "            if bestdevloss>min(devLoss_history):\n",
        "              print(\" ----------------------------------------------------------------- new best found ^.^\")\n",
        "              bestdevloss=devLoss\n",
        "              bestwholeloss=wholeLoss\n",
        "              bestepoch=lastepoch\n",
        "              besttestloss1,besttestloss2=getTestLoss()\n",
        "              \n",
        "            if (devLoss-wholeLoss)>overfitcutoff:\n",
        "              print('Stoppping due to overfit')\n",
        "              overfitFlag=True\n",
        "              break\n",
        "    print(\"Epoch {:03d}/{:03d} completed \\t - \\tLoss at batchSetNum {:02d}: {:.9f}\".format((epoch), maxEpochsforthis-1, batch, wholeLoss),np.round(((dt.now()-starttime).seconds)/60.0,1), \"mins\")\n",
        "print(\"Final loss: {:.9f}\".format(wholeLoss),np.round(((dt.now()-starttime).seconds)/60.0,1), \"mins\")\n",
        "print('total epochs run so far(starts from 0):',lastepoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New Epoch Started: 0\n",
            "\tEpoch 000/009: Loss at batchSetNum 00: 0.380821794 0.0 mins\n",
            "\t               WholeLoss: 0.38082 DevLoss: 0.36582\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.21022432\n",
            "Seq: 3 Size: 640 loss: 0.3325615\n",
            "Seq: 4 Size: 640 loss: 0.42505035\n",
            "Seq: 5 Size: 640 loss: 0.49624684\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.5499851\n",
            "Seq: 7 Size: 640 loss: 0.59457\n",
            "Seq: 8 Size: 640 loss: 0.6330074\n",
            "Seq: 9 Size: 640 loss: 0.6677626\n",
            "Seq: 10 Size: 640 loss: 0.69355917\n",
            "loss1: 0.366021\n",
            "loss2: 0.627777\n",
            "\tEpoch 000/009: Loss at batchSetNum 10: 0.299850285 0.1 mins\n",
            "\t               WholeLoss: 0.29985 DevLoss: 0.29995\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.1715949\n",
            "Seq: 3 Size: 640 loss: 0.27407372\n",
            "Seq: 4 Size: 640 loss: 0.348507\n",
            "Seq: 5 Size: 640 loss: 0.40324715\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.44961694\n",
            "Seq: 7 Size: 640 loss: 0.4889637\n",
            "Seq: 8 Size: 640 loss: 0.52253014\n",
            "Seq: 9 Size: 640 loss: 0.5520216\n",
            "Seq: 10 Size: 640 loss: 0.57814455\n",
            "loss1: 0.299356\n",
            "loss2: 0.518255\n",
            "\tEpoch 000/009: Loss at batchSetNum 20: 0.298816174 0.2 mins\n",
            "\t               WholeLoss: 0.29882 DevLoss: 0.29961\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.17112511\n",
            "Seq: 3 Size: 640 loss: 0.2739277\n",
            "Seq: 4 Size: 640 loss: 0.34757888\n",
            "Seq: 5 Size: 640 loss: 0.40563095\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.4517236\n",
            "Seq: 7 Size: 640 loss: 0.49030066\n",
            "Seq: 8 Size: 640 loss: 0.5238162\n",
            "Seq: 9 Size: 640 loss: 0.5530976\n",
            "Seq: 10 Size: 640 loss: 0.5793132\n",
            "loss1: 0.299566\n",
            "loss2: 0.51965\n",
            "\tEpoch 000/009: Loss at batchSetNum 30: 0.299158245 0.3 mins\n",
            "\t               WholeLoss: 0.29916 DevLoss: 0.30045\n",
            "\tEpoch 000/009: Loss at batchSetNum 40: 0.311363250 0.4 mins\n",
            "\t               WholeLoss: 0.31136 DevLoss: 0.30172\n",
            "\tEpoch 000/009: Loss at batchSetNum 50: 0.302224398 0.5 mins\n",
            "\t               WholeLoss: 0.30222 DevLoss: 0.29985\n",
            "\tEpoch 000/009: Loss at batchSetNum 60: 0.299029440 0.6 mins\n",
            "\t               WholeLoss: 0.29903 DevLoss: 0.29936\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.16920193\n",
            "Seq: 3 Size: 640 loss: 0.2725962\n",
            "Seq: 4 Size: 640 loss: 0.3486343\n",
            "Seq: 5 Size: 640 loss: 0.40688446\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.45451584\n",
            "Seq: 7 Size: 640 loss: 0.4931586\n",
            "Seq: 8 Size: 640 loss: 0.52687204\n",
            "Seq: 9 Size: 640 loss: 0.55623084\n",
            "Seq: 10 Size: 640 loss: 0.5828803\n",
            "loss1: 0.299329\n",
            "loss2: 0.522731\n",
            "\tEpoch 000/009: Loss at batchSetNum 70: 0.297706664 0.7 mins\n",
            "\t               WholeLoss: 0.29771 DevLoss: 0.29894\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.16409378\n",
            "Seq: 3 Size: 640 loss: 0.27285385\n",
            "Seq: 4 Size: 640 loss: 0.35054302\n",
            "Seq: 5 Size: 640 loss: 0.40902665\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.4569984\n",
            "Seq: 7 Size: 640 loss: 0.49632084\n",
            "Seq: 8 Size: 640 loss: 0.5310671\n",
            "Seq: 9 Size: 640 loss: 0.56080896\n",
            "Seq: 10 Size: 640 loss: 0.588021\n",
            "loss1: 0.299129\n",
            "loss2: 0.526643\n",
            "\tEpoch 000/009: Loss at batchSetNum 80: 0.295971632 0.8 mins\n",
            "\t               WholeLoss: 0.29597 DevLoss: 0.29921\n",
            "\tEpoch 000/009: Loss at batchSetNum 90: 0.290314496 0.9 mins\n",
            "\t               WholeLoss: 0.29031 DevLoss: 0.29568\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.15666528\n",
            "Seq: 3 Size: 640 loss: 0.2707145\n",
            "Seq: 4 Size: 640 loss: 0.34959096\n",
            "Seq: 5 Size: 640 loss: 0.40637743\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.45336297\n",
            "Seq: 7 Size: 640 loss: 0.49104756\n",
            "Seq: 8 Size: 640 loss: 0.5240114\n",
            "Seq: 9 Size: 640 loss: 0.5529628\n",
            "Seq: 10 Size: 640 loss: 0.581205\n",
            "loss1: 0.295837\n",
            "loss2: 0.520518\n",
            "\tEpoch 000/009: Loss at batchSetNum 100: 0.290104210 1.0 mins\n",
            "\t               WholeLoss: 0.2901 DevLoss: 0.29534\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.15569566\n",
            "Seq: 3 Size: 640 loss: 0.26814607\n",
            "Seq: 4 Size: 640 loss: 0.35038787\n",
            "Seq: 5 Size: 640 loss: 0.40662724\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.45287636\n",
            "Seq: 7 Size: 640 loss: 0.49181366\n",
            "Seq: 8 Size: 640 loss: 0.52640307\n",
            "Seq: 9 Size: 640 loss: 0.5570093\n",
            "Seq: 10 Size: 640 loss: 0.5829745\n",
            "loss1: 0.295214\n",
            "loss2: 0.522215\n",
            "\tEpoch 000/009: Loss at batchSetNum 110: 0.302096009 1.1 mins\n",
            "\t               WholeLoss: 0.3021 DevLoss: 0.29766\n",
            "\tEpoch 000/009: Loss at batchSetNum 120: 0.292796433 1.2 mins\n",
            "\t               WholeLoss: 0.2928 DevLoss: 0.29656\n",
            "\tEpoch 000/009: Loss at batchSetNum 130: 0.297431231 1.3 mins\n",
            "\t               WholeLoss: 0.29743 DevLoss: 0.29531\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.15700978\n",
            "Seq: 3 Size: 640 loss: 0.2693486\n",
            "Seq: 4 Size: 640 loss: 0.3487844\n",
            "Seq: 5 Size: 640 loss: 0.4074578\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.4546815\n",
            "Seq: 7 Size: 640 loss: 0.49306718\n",
            "Seq: 8 Size: 640 loss: 0.527095\n",
            "Seq: 9 Size: 640 loss: 0.55631936\n",
            "Seq: 10 Size: 640 loss: 0.58371854\n",
            "loss1: 0.29565\n",
            "loss2: 0.522976\n",
            "\tEpoch 000/009: Loss at batchSetNum 140: 0.290252924 1.4 mins\n",
            "\t               WholeLoss: 0.29025 DevLoss: 0.29254\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.15349087\n",
            "Seq: 3 Size: 640 loss: 0.26606992\n",
            "Seq: 4 Size: 640 loss: 0.3457117\n",
            "Seq: 5 Size: 640 loss: 0.40321416\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.45199338\n",
            "Seq: 7 Size: 640 loss: 0.4897103\n",
            "Seq: 8 Size: 640 loss: 0.52266014\n",
            "Seq: 9 Size: 640 loss: 0.552321\n",
            "Seq: 10 Size: 640 loss: 0.57780296\n",
            "loss1: 0.292122\n",
            "loss2: 0.518898\n",
            "\tEpoch 000/009: Loss at batchSetNum 150: 0.291232765 1.5 mins\n",
            "\t               WholeLoss: 0.29123 DevLoss: 0.29124\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.15132037\n",
            "Seq: 3 Size: 640 loss: 0.26127914\n",
            "Seq: 4 Size: 640 loss: 0.34255496\n",
            "Seq: 5 Size: 640 loss: 0.40571883\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.45290297\n",
            "Seq: 7 Size: 640 loss: 0.4909197\n",
            "Seq: 8 Size: 640 loss: 0.5241842\n",
            "Seq: 9 Size: 640 loss: 0.55376565\n",
            "Seq: 10 Size: 640 loss: 0.5803223\n",
            "loss1: 0.290218\n",
            "loss2: 0.520419\n",
            "\tEpoch 000/009: Loss at batchSetNum 160: 0.288497627 1.6 mins\n",
            "\t               WholeLoss: 0.2885 DevLoss: 0.29415\n",
            "\tEpoch 000/009: Loss at batchSetNum 170: 0.290433168 1.7 mins\n",
            "\t               WholeLoss: 0.29043 DevLoss: 0.29155\n",
            "\tEpoch 000/009: Loss at batchSetNum 180: 0.285689950 1.8 mins\n",
            "\t               WholeLoss: 0.28569 DevLoss: 0.29002\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.1396625\n",
            "Seq: 3 Size: 640 loss: 0.2653319\n",
            "Seq: 4 Size: 640 loss: 0.34836787\n",
            "Seq: 5 Size: 640 loss: 0.4062041\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.45235497\n",
            "Seq: 7 Size: 640 loss: 0.4913949\n",
            "Seq: 8 Size: 640 loss: 0.5234147\n",
            "Seq: 9 Size: 640 loss: 0.55297756\n",
            "Seq: 10 Size: 640 loss: 0.5791547\n",
            "loss1: 0.289892\n",
            "loss2: 0.519859\n",
            "\tEpoch 000/009: Loss at batchSetNum 190: 0.288128912 1.9 mins\n",
            "\t               WholeLoss: 0.28813 DevLoss: 0.29034\n",
            "Epoch 000/009 completed \t - \tLoss at batchSetNum 199: 0.286898553 2.0 mins\n",
            "New Epoch Started: 1\n",
            "\tEpoch 001/009: Loss at batchSetNum 00: 0.285961002 2.0 mins\n",
            "\t               WholeLoss: 0.28596 DevLoss: 0.28658\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.14334354\n",
            "Seq: 3 Size: 640 loss: 0.2568665\n",
            "Seq: 4 Size: 640 loss: 0.34067044\n",
            "Seq: 5 Size: 640 loss: 0.4065712\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.45400724\n",
            "Seq: 7 Size: 640 loss: 0.49074507\n",
            "Seq: 8 Size: 640 loss: 0.52491796\n",
            "Seq: 9 Size: 640 loss: 0.55338585\n",
            "Seq: 10 Size: 640 loss: 0.57947356\n",
            "loss1: 0.286863\n",
            "loss2: 0.520506\n",
            "\tEpoch 001/009: Loss at batchSetNum 10: 0.282512993 2.1 mins\n",
            "\t               WholeLoss: 0.28251 DevLoss: 0.28766\n",
            "\tEpoch 001/009: Loss at batchSetNum 20: 0.283183455 2.2 mins\n",
            "\t               WholeLoss: 0.28318 DevLoss: 0.28705\n",
            "\tEpoch 001/009: Loss at batchSetNum 30: 0.285658985 2.3 mins\n",
            "\t               WholeLoss: 0.28566 DevLoss: 0.28706\n",
            "\tEpoch 001/009: Loss at batchSetNum 40: 0.277738988 2.4 mins\n",
            "\t               WholeLoss: 0.27774 DevLoss: 0.28485\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.13786532\n",
            "Seq: 3 Size: 640 loss: 0.25554797\n",
            "Seq: 4 Size: 640 loss: 0.33778083\n",
            "Seq: 5 Size: 640 loss: 0.40767193\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.456225\n",
            "Seq: 7 Size: 640 loss: 0.4915868\n",
            "Seq: 8 Size: 640 loss: 0.52618694\n",
            "Seq: 9 Size: 640 loss: 0.5545673\n",
            "Seq: 10 Size: 640 loss: 0.5798517\n",
            "loss1: 0.284717\n",
            "loss2: 0.521684\n",
            "\tEpoch 001/009: Loss at batchSetNum 50: 0.300440907 2.5 mins\n",
            "\t               WholeLoss: 0.30044 DevLoss: 0.28373\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.13741413\n",
            "Seq: 3 Size: 640 loss: 0.25580496\n",
            "Seq: 4 Size: 640 loss: 0.33724266\n",
            "Seq: 5 Size: 640 loss: 0.40826032\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.45380077\n",
            "Seq: 7 Size: 640 loss: 0.49148467\n",
            "Seq: 8 Size: 640 loss: 0.524356\n",
            "Seq: 9 Size: 640 loss: 0.55408853\n",
            "Seq: 10 Size: 640 loss: 0.57960755\n",
            "loss1: 0.284681\n",
            "loss2: 0.520668\n",
            "\tEpoch 001/009: Loss at batchSetNum 60: 0.284748822 2.6 mins\n",
            "\t               WholeLoss: 0.28475 DevLoss: 0.28522\n",
            "\tEpoch 001/009: Loss at batchSetNum 70: 0.283389926 2.7 mins\n",
            "\t               WholeLoss: 0.28339 DevLoss: 0.28366\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.133938\n",
            "Seq: 3 Size: 640 loss: 0.2550427\n",
            "Seq: 4 Size: 640 loss: 0.33892187\n",
            "Seq: 5 Size: 640 loss: 0.4061065\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.45283672\n",
            "Seq: 7 Size: 640 loss: 0.48987785\n",
            "Seq: 8 Size: 640 loss: 0.5245226\n",
            "Seq: 9 Size: 640 loss: 0.553068\n",
            "Seq: 10 Size: 640 loss: 0.5802084\n",
            "loss1: 0.283502\n",
            "loss2: 0.520103\n",
            "\tEpoch 001/009: Loss at batchSetNum 80: 0.278053671 2.8 mins\n",
            "\t               WholeLoss: 0.27805 DevLoss: 0.27992\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.12416061\n",
            "Seq: 3 Size: 640 loss: 0.2514184\n",
            "Seq: 4 Size: 640 loss: 0.33575845\n",
            "Seq: 5 Size: 640 loss: 0.4058422\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.45508873\n",
            "Seq: 7 Size: 640 loss: 0.49307147\n",
            "Seq: 8 Size: 640 loss: 0.52596015\n",
            "Seq: 9 Size: 640 loss: 0.5545202\n",
            "Seq: 10 Size: 640 loss: 0.5805464\n",
            "loss1: 0.279295\n",
            "loss2: 0.521837\n",
            "\tEpoch 001/009: Loss at batchSetNum 90: 0.280380994 2.9 mins\n",
            "\t               WholeLoss: 0.28038 DevLoss: 0.28228\n",
            "\tEpoch 001/009: Loss at batchSetNum 100: 0.275093496 3.0 mins\n",
            "\t               WholeLoss: 0.27509 DevLoss: 0.2854\n",
            "\tEpoch 001/009: Loss at batchSetNum 110: 0.277842462 3.0 mins\n",
            "\t               WholeLoss: 0.27784 DevLoss: 0.28409\n",
            "\tEpoch 001/009: Loss at batchSetNum 120: 0.279118240 3.1 mins\n",
            "\t               WholeLoss: 0.27912 DevLoss: 0.28679\n",
            "\tEpoch 001/009: Loss at batchSetNum 130: 0.277490646 3.2 mins\n",
            "\t               WholeLoss: 0.27749 DevLoss: 0.284\n",
            "\tEpoch 001/009: Loss at batchSetNum 140: 0.283803463 3.3 mins\n",
            "\t               WholeLoss: 0.2838 DevLoss: 0.28594\n",
            "\tEpoch 001/009: Loss at batchSetNum 150: 0.283267289 3.4 mins\n",
            "\t               WholeLoss: 0.28327 DevLoss: 0.28397\n",
            "\tEpoch 001/009: Loss at batchSetNum 160: 0.277315915 3.5 mins\n",
            "\t               WholeLoss: 0.27732 DevLoss: 0.28869\n",
            "\tEpoch 001/009: Loss at batchSetNum 170: 0.274109691 3.6 mins\n",
            "\t               WholeLoss: 0.27411 DevLoss: 0.28242\n",
            "\tEpoch 001/009: Loss at batchSetNum 180: 0.270202696 3.6 mins\n",
            "\t               WholeLoss: 0.2702 DevLoss: 0.27701\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.13042071\n",
            "Seq: 3 Size: 640 loss: 0.24645391\n",
            "Seq: 4 Size: 640 loss: 0.33334345\n",
            "Seq: 5 Size: 640 loss: 0.40842292\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.45476907\n",
            "Seq: 7 Size: 640 loss: 0.49205583\n",
            "Seq: 8 Size: 640 loss: 0.5259032\n",
            "Seq: 9 Size: 640 loss: 0.55466324\n",
            "Seq: 10 Size: 640 loss: 0.57969177\n",
            "loss1: 0.27966\n",
            "loss2: 0.521417\n",
            "\tEpoch 001/009: Loss at batchSetNum 190: 0.280376881 3.8 mins\n",
            "\t               WholeLoss: 0.28038 DevLoss: 0.28272\n",
            "Epoch 001/009 completed \t - \tLoss at batchSetNum 199: 0.283278584 3.8 mins\n",
            "New Epoch Started: 2\n",
            "\tEpoch 002/009: Loss at batchSetNum 00: 0.282345027 3.8 mins\n",
            "\t               WholeLoss: 0.28235 DevLoss: 0.28517\n",
            "\tEpoch 002/009: Loss at batchSetNum 10: 0.281450868 3.9 mins\n",
            "\t               WholeLoss: 0.28145 DevLoss: 0.28506\n",
            "\tEpoch 002/009: Loss at batchSetNum 20: 0.281237751 4.0 mins\n",
            "\t               WholeLoss: 0.28124 DevLoss: 0.28523\n",
            "\tEpoch 002/009: Loss at batchSetNum 30: 0.281539738 4.1 mins\n",
            "\t               WholeLoss: 0.28154 DevLoss: 0.27866\n",
            "\tEpoch 002/009: Loss at batchSetNum 40: 0.273994267 4.2 mins\n",
            "\t               WholeLoss: 0.27399 DevLoss: 0.27938\n",
            "\tEpoch 002/009: Loss at batchSetNum 50: 0.297782779 4.3 mins\n",
            "\t               WholeLoss: 0.29778 DevLoss: 0.27889\n",
            "\tEpoch 002/009: Loss at batchSetNum 60: 0.272261620 4.4 mins\n",
            "\t               WholeLoss: 0.27226 DevLoss: 0.27957\n",
            "\tEpoch 002/009: Loss at batchSetNum 70: 0.284565777 4.4 mins\n",
            "\t               WholeLoss: 0.28457 DevLoss: 0.27881\n",
            "\tEpoch 002/009: Loss at batchSetNum 80: 0.274675041 4.5 mins\n",
            "\t               WholeLoss: 0.27468 DevLoss: 0.28128\n",
            "\tEpoch 002/009: Loss at batchSetNum 90: 0.276160538 4.6 mins\n",
            "\t               WholeLoss: 0.27616 DevLoss: 0.28297\n",
            "\tEpoch 002/009: Loss at batchSetNum 100: 0.273709118 4.7 mins\n",
            "\t               WholeLoss: 0.27371 DevLoss: 0.28187\n",
            "\tEpoch 002/009: Loss at batchSetNum 110: 0.276353896 4.8 mins\n",
            "\t               WholeLoss: 0.27635 DevLoss: 0.28251\n",
            "\tEpoch 002/009: Loss at batchSetNum 120: 0.274010837 4.9 mins\n",
            "\t               WholeLoss: 0.27401 DevLoss: 0.28393\n",
            "\tEpoch 002/009: Loss at batchSetNum 130: 0.269654006 5.0 mins\n",
            "\t               WholeLoss: 0.26965 DevLoss: 0.28379\n",
            "\tEpoch 002/009: Loss at batchSetNum 140: 0.285014629 5.0 mins\n",
            "\t               WholeLoss: 0.28501 DevLoss: 0.28524\n",
            "\tEpoch 002/009: Loss at batchSetNum 150: 0.277626902 5.2 mins\n",
            "\t               WholeLoss: 0.27763 DevLoss: 0.27722\n",
            "\tEpoch 002/009: Loss at batchSetNum 160: 0.275185227 5.2 mins\n",
            "\t               WholeLoss: 0.27519 DevLoss: 0.28814\n",
            "\tEpoch 002/009: Loss at batchSetNum 170: 0.271276712 5.3 mins\n",
            "\t               WholeLoss: 0.27128 DevLoss: 0.27577\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.12831435\n",
            "Seq: 3 Size: 640 loss: 0.2490216\n",
            "Seq: 4 Size: 640 loss: 0.33031192\n",
            "Seq: 5 Size: 640 loss: 0.39827824\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.4501634\n",
            "Seq: 7 Size: 640 loss: 0.49198526\n",
            "Seq: 8 Size: 640 loss: 0.5242509\n",
            "Seq: 9 Size: 640 loss: 0.55340785\n",
            "Seq: 10 Size: 640 loss: 0.5795412\n",
            "loss1: 0.276482\n",
            "loss2: 0.51987\n",
            "\tEpoch 002/009: Loss at batchSetNum 180: 0.276564777 5.4 mins\n",
            "\t               WholeLoss: 0.27656 DevLoss: 0.27691\n",
            "\tEpoch 002/009: Loss at batchSetNum 190: 0.279571176 5.5 mins\n",
            "\t               WholeLoss: 0.27957 DevLoss: 0.27462\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.12770408\n",
            "Seq: 3 Size: 640 loss: 0.24821554\n",
            "Seq: 4 Size: 640 loss: 0.33197865\n",
            "Seq: 5 Size: 640 loss: 0.39389342\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.44500306\n",
            "Seq: 7 Size: 640 loss: 0.4927287\n",
            "Seq: 8 Size: 640 loss: 0.5257193\n",
            "Seq: 9 Size: 640 loss: 0.55440116\n",
            "Seq: 10 Size: 640 loss: 0.58042246\n",
            "loss1: 0.275448\n",
            "loss2: 0.519655\n",
            "Epoch 002/009 completed \t - \tLoss at batchSetNum 199: 0.280197322 5.6 mins\n",
            "New Epoch Started: 3\n",
            "\tEpoch 003/009: Loss at batchSetNum 00: 0.279369861 5.6 mins\n",
            "\t               WholeLoss: 0.27937 DevLoss: 0.2753\n",
            "\tEpoch 003/009: Loss at batchSetNum 10: 0.274127007 5.7 mins\n",
            "\t               WholeLoss: 0.27413 DevLoss: 0.27686\n",
            "\tEpoch 003/009: Loss at batchSetNum 20: 0.279628694 5.8 mins\n",
            "\t               WholeLoss: 0.27963 DevLoss: 0.28565\n",
            "\tEpoch 003/009: Loss at batchSetNum 30: 0.280805290 5.9 mins\n",
            "\t               WholeLoss: 0.28081 DevLoss: 0.28284\n",
            "\tEpoch 003/009: Loss at batchSetNum 40: 0.274729967 6.0 mins\n",
            "\t               WholeLoss: 0.27473 DevLoss: 0.28095\n",
            "\tEpoch 003/009: Loss at batchSetNum 50: 0.296968013 6.1 mins\n",
            "\t               WholeLoss: 0.29697 DevLoss: 0.27629\n",
            "\tEpoch 003/009: Loss at batchSetNum 60: 0.273260832 6.2 mins\n",
            "\t               WholeLoss: 0.27326 DevLoss: 0.28432\n",
            "\tEpoch 003/009: Loss at batchSetNum 70: 0.279498428 6.3 mins\n",
            "\t               WholeLoss: 0.2795 DevLoss: 0.28187\n",
            "\tEpoch 003/009: Loss at batchSetNum 80: 0.277232319 6.4 mins\n",
            "\t               WholeLoss: 0.27723 DevLoss: 0.2788\n",
            "\tEpoch 003/009: Loss at batchSetNum 90: 0.274412572 6.4 mins\n",
            "\t               WholeLoss: 0.27441 DevLoss: 0.28242\n",
            "\tEpoch 003/009: Loss at batchSetNum 100: 0.271623075 6.5 mins\n",
            "\t               WholeLoss: 0.27162 DevLoss: 0.28095\n",
            "\tEpoch 003/009: Loss at batchSetNum 110: 0.276086658 6.6 mins\n",
            "\t               WholeLoss: 0.27609 DevLoss: 0.28339\n",
            "\tEpoch 003/009: Loss at batchSetNum 120: 0.272956491 6.7 mins\n",
            "\t               WholeLoss: 0.27296 DevLoss: 0.28205\n",
            "\tEpoch 003/009: Loss at batchSetNum 130: 0.266092479 6.8 mins\n",
            "\t               WholeLoss: 0.26609 DevLoss: 0.28158\n",
            "\tEpoch 003/009: Loss at batchSetNum 140: 0.283077866 6.8 mins\n",
            "\t               WholeLoss: 0.28308 DevLoss: 0.28369\n",
            "\tEpoch 003/009: Loss at batchSetNum 150: 0.277692020 6.9 mins\n",
            "\t               WholeLoss: 0.27769 DevLoss: 0.28073\n",
            "\tEpoch 003/009: Loss at batchSetNum 160: 0.270190448 7.0 mins\n",
            "\t               WholeLoss: 0.27019 DevLoss: 0.28274\n",
            "\tEpoch 003/009: Loss at batchSetNum 170: 0.269288063 7.1 mins\n",
            "\t               WholeLoss: 0.26929 DevLoss: 0.28173\n",
            "\tEpoch 003/009: Loss at batchSetNum 180: 0.266765296 7.2 mins\n",
            "\t               WholeLoss: 0.26677 DevLoss: 0.28022\n",
            "\tEpoch 003/009: Loss at batchSetNum 190: 0.282595247 7.3 mins\n",
            "\t               WholeLoss: 0.2826 DevLoss: 0.28454\n",
            "Epoch 003/009 completed \t - \tLoss at batchSetNum 199: 0.284794688 7.4 mins\n",
            "New Epoch Started: 4\n",
            "\tEpoch 004/009: Loss at batchSetNum 00: 0.280018955 7.4 mins\n",
            "\t               WholeLoss: 0.28002 DevLoss: 0.28473\n",
            "\tEpoch 004/009: Loss at batchSetNum 10: 0.272614837 7.5 mins\n",
            "\t               WholeLoss: 0.27261 DevLoss: 0.28633\n",
            "\tEpoch 004/009: Loss at batchSetNum 20: 0.277350008 7.6 mins\n",
            "\t               WholeLoss: 0.27735 DevLoss: 0.28393\n",
            "\tEpoch 004/009: Loss at batchSetNum 30: 0.280483365 7.6 mins\n",
            "\t               WholeLoss: 0.28048 DevLoss: 0.28214\n",
            "\tEpoch 004/009: Loss at batchSetNum 40: 0.275075704 7.7 mins\n",
            "\t               WholeLoss: 0.27508 DevLoss: 0.28176\n",
            "\tEpoch 004/009: Loss at batchSetNum 50: 0.294337332 7.8 mins\n",
            "\t               WholeLoss: 0.29434 DevLoss: 0.2766\n",
            "\tEpoch 004/009: Loss at batchSetNum 60: 0.270766735 7.9 mins\n",
            "\t               WholeLoss: 0.27077 DevLoss: 0.2778\n",
            "\tEpoch 004/009: Loss at batchSetNum 70: 0.279195786 8.0 mins\n",
            "\t               WholeLoss: 0.2792 DevLoss: 0.28079\n",
            "\tEpoch 004/009: Loss at batchSetNum 80: 0.271364272 8.0 mins\n",
            "\t               WholeLoss: 0.27136 DevLoss: 0.28022\n",
            "\tEpoch 004/009: Loss at batchSetNum 90: 0.274184883 8.1 mins\n",
            "\t               WholeLoss: 0.27418 DevLoss: 0.28256\n",
            "\tEpoch 004/009: Loss at batchSetNum 100: 0.272167921 8.2 mins\n",
            "\t               WholeLoss: 0.27217 DevLoss: 0.28204\n",
            "\tEpoch 004/009: Loss at batchSetNum 110: 0.273597181 8.3 mins\n",
            "\t               WholeLoss: 0.2736 DevLoss: 0.28264\n",
            "\tEpoch 004/009: Loss at batchSetNum 120: 0.277559966 8.4 mins\n",
            "\t               WholeLoss: 0.27756 DevLoss: 0.27672\n",
            "\tEpoch 004/009: Loss at batchSetNum 130: 0.269223630 8.5 mins\n",
            "\t               WholeLoss: 0.26922 DevLoss: 0.27918\n",
            "\tEpoch 004/009: Loss at batchSetNum 140: 0.285416663 8.6 mins\n",
            "\t               WholeLoss: 0.28542 DevLoss: 0.27828\n",
            "\tEpoch 004/009: Loss at batchSetNum 150: 0.280134916 8.7 mins\n",
            "\t               WholeLoss: 0.28013 DevLoss: 0.28303\n",
            "\tEpoch 004/009: Loss at batchSetNum 160: 0.272292137 8.8 mins\n",
            "\t               WholeLoss: 0.27229 DevLoss: 0.28714\n",
            "\tEpoch 004/009: Loss at batchSetNum 170: 0.272880316 8.8 mins\n",
            "\t               WholeLoss: 0.27288 DevLoss: 0.28169\n",
            "\tEpoch 004/009: Loss at batchSetNum 180: 0.266710997 8.9 mins\n",
            "\t               WholeLoss: 0.26671 DevLoss: 0.2831\n",
            "\tEpoch 004/009: Loss at batchSetNum 190: 0.278383404 9.0 mins\n",
            "\t               WholeLoss: 0.27838 DevLoss: 0.28129\n",
            "Epoch 004/009 completed \t - \tLoss at batchSetNum 199: 0.279475570 9.1 mins\n",
            "New Epoch Started: 5\n",
            "\tEpoch 005/009: Loss at batchSetNum 00: 0.278204650 9.1 mins\n",
            "\t               WholeLoss: 0.2782 DevLoss: 0.28327\n",
            "\tEpoch 005/009: Loss at batchSetNum 10: 0.273202240 9.2 mins\n",
            "\t               WholeLoss: 0.2732 DevLoss: 0.28462\n",
            "\tEpoch 005/009: Loss at batchSetNum 20: 0.278280526 9.2 mins\n",
            "\t               WholeLoss: 0.27828 DevLoss: 0.28358\n",
            "\tEpoch 005/009: Loss at batchSetNum 30: 0.278644770 9.3 mins\n",
            "\t               WholeLoss: 0.27864 DevLoss: 0.28338\n",
            "\tEpoch 005/009: Loss at batchSetNum 40: 0.273293197 9.4 mins\n",
            "\t               WholeLoss: 0.27329 DevLoss: 0.28107\n",
            "\tEpoch 005/009: Loss at batchSetNum 50: 0.291442305 9.5 mins\n",
            "\t               WholeLoss: 0.29144 DevLoss: 0.28093\n",
            "\tEpoch 005/009: Loss at batchSetNum 60: 0.269104838 9.6 mins\n",
            "\t               WholeLoss: 0.2691 DevLoss: 0.2818\n",
            "\tEpoch 005/009: Loss at batchSetNum 70: 0.278328538 9.7 mins\n",
            "\t               WholeLoss: 0.27833 DevLoss: 0.28254\n",
            "\tEpoch 005/009: Loss at batchSetNum 80: 0.269948095 9.8 mins\n",
            "\t               WholeLoss: 0.26995 DevLoss: 0.28202\n",
            "\tEpoch 005/009: Loss at batchSetNum 90: 0.271674573 9.8 mins\n",
            "\t               WholeLoss: 0.27167 DevLoss: 0.28359\n",
            "\tEpoch 005/009: Loss at batchSetNum 100: 0.271247983 10.0 mins\n",
            "\t               WholeLoss: 0.27125 DevLoss: 0.27953\n",
            "\tEpoch 005/009: Loss at batchSetNum 110: 0.273822784 10.0 mins\n",
            "\t               WholeLoss: 0.27382 DevLoss: 0.27974\n",
            "\tEpoch 005/009: Loss at batchSetNum 120: 0.271025658 10.1 mins\n",
            "\t               WholeLoss: 0.27103 DevLoss: 0.27855\n",
            "\tEpoch 005/009: Loss at batchSetNum 130: 0.262400210 10.2 mins\n",
            "\t               WholeLoss: 0.2624 DevLoss: 0.28229\n",
            "\tEpoch 005/009: Loss at batchSetNum 140: 0.284423709 10.3 mins\n",
            "\t               WholeLoss: 0.28442 DevLoss: 0.28297\n",
            "\tEpoch 005/009: Loss at batchSetNum 150: 0.281681091 10.4 mins\n",
            "\t               WholeLoss: 0.28168 DevLoss: 0.28208\n",
            "\tEpoch 005/009: Loss at batchSetNum 160: 0.273436785 10.5 mins\n",
            "\t               WholeLoss: 0.27344 DevLoss: 0.28604\n",
            "\tEpoch 005/009: Loss at batchSetNum 170: 0.271358043 10.6 mins\n",
            "\t               WholeLoss: 0.27136 DevLoss: 0.28371\n",
            "\tEpoch 005/009: Loss at batchSetNum 180: 0.266446114 10.6 mins\n",
            "\t               WholeLoss: 0.26645 DevLoss: 0.2795\n",
            "\tEpoch 005/009: Loss at batchSetNum 190: 0.275732905 10.7 mins\n",
            "\t               WholeLoss: 0.27573 DevLoss: 0.2825\n",
            "Epoch 005/009 completed \t - \tLoss at batchSetNum 199: 0.279133737 10.8 mins\n",
            "New Epoch Started: 6\n",
            "\tEpoch 006/009: Loss at batchSetNum 00: 0.278263241 10.8 mins\n",
            "\t               WholeLoss: 0.27826 DevLoss: 0.28352\n",
            "\tEpoch 006/009: Loss at batchSetNum 10: 0.271983415 10.9 mins\n",
            "\t               WholeLoss: 0.27198 DevLoss: 0.28339\n",
            "\tEpoch 006/009: Loss at batchSetNum 20: 0.276931792 11.0 mins\n",
            "\t               WholeLoss: 0.27693 DevLoss: 0.28306\n",
            "\tEpoch 006/009: Loss at batchSetNum 30: 0.278529704 11.1 mins\n",
            "\t               WholeLoss: 0.27853 DevLoss: 0.28273\n",
            "\tEpoch 006/009: Loss at batchSetNum 40: 0.275337130 11.2 mins\n",
            "\t               WholeLoss: 0.27534 DevLoss: 0.2777\n",
            "\tEpoch 006/009: Loss at batchSetNum 50: 0.289747953 11.2 mins\n",
            "\t               WholeLoss: 0.28975 DevLoss: 0.28135\n",
            "\tEpoch 006/009: Loss at batchSetNum 60: 0.269758970 11.3 mins\n",
            "\t               WholeLoss: 0.26976 DevLoss: 0.28004\n",
            "\tEpoch 006/009: Loss at batchSetNum 70: 0.277693123 11.4 mins\n",
            "\t               WholeLoss: 0.27769 DevLoss: 0.27717\n",
            "\tEpoch 006/009: Loss at batchSetNum 80: 0.269129485 11.5 mins\n",
            "\t               WholeLoss: 0.26913 DevLoss: 0.27576\n",
            "\tEpoch 006/009: Loss at batchSetNum 90: 0.271883309 11.6 mins\n",
            "\t               WholeLoss: 0.27188 DevLoss: 0.28324\n",
            "\tEpoch 006/009: Loss at batchSetNum 100: 0.269796848 11.7 mins\n",
            "\t               WholeLoss: 0.2698 DevLoss: 0.28093\n",
            "\tEpoch 006/009: Loss at batchSetNum 110: 0.272910446 11.8 mins\n",
            "\t               WholeLoss: 0.27291 DevLoss: 0.28049\n",
            "\tEpoch 006/009: Loss at batchSetNum 120: 0.271079510 11.8 mins\n",
            "\t               WholeLoss: 0.27108 DevLoss: 0.28224\n",
            "\tEpoch 006/009: Loss at batchSetNum 130: 0.268853217 11.9 mins\n",
            "\t               WholeLoss: 0.26885 DevLoss: 0.28216\n",
            "\tEpoch 006/009: Loss at batchSetNum 140: 0.279095948 12.0 mins\n",
            "\t               WholeLoss: 0.2791 DevLoss: 0.27696\n",
            "\tEpoch 006/009: Loss at batchSetNum 150: 0.278457403 12.1 mins\n",
            "\t               WholeLoss: 0.27846 DevLoss: 0.28304\n",
            "\tEpoch 006/009: Loss at batchSetNum 160: 0.268422604 12.2 mins\n",
            "\t               WholeLoss: 0.26842 DevLoss: 0.27538\n",
            "\tEpoch 006/009: Loss at batchSetNum 170: 0.269556671 12.2 mins\n",
            "\t               WholeLoss: 0.26956 DevLoss: 0.28222\n",
            "\tEpoch 006/009: Loss at batchSetNum 180: 0.267036438 12.3 mins\n",
            "\t               WholeLoss: 0.26704 DevLoss: 0.28103\n",
            "\tEpoch 006/009: Loss at batchSetNum 190: 0.273339719 12.4 mins\n",
            "\t               WholeLoss: 0.27334 DevLoss: 0.27902\n",
            "Epoch 006/009 completed \t - \tLoss at batchSetNum 199: 0.276551545 12.5 mins\n",
            "New Epoch Started: 7\n",
            "\tEpoch 007/009: Loss at batchSetNum 00: 0.278590322 12.5 mins\n",
            "\t               WholeLoss: 0.27859 DevLoss: 0.28397\n",
            "\tEpoch 007/009: Loss at batchSetNum 10: 0.269063711 12.6 mins\n",
            "\t               WholeLoss: 0.26906 DevLoss: 0.28382\n",
            "\tEpoch 007/009: Loss at batchSetNum 20: 0.274585456 12.7 mins\n",
            "\t               WholeLoss: 0.27459 DevLoss: 0.28738\n",
            "\tEpoch 007/009: Loss at batchSetNum 30: 0.276252478 12.8 mins\n",
            "\t               WholeLoss: 0.27625 DevLoss: 0.27879\n",
            "\tEpoch 007/009: Loss at batchSetNum 40: 0.271355838 12.8 mins\n",
            "\t               WholeLoss: 0.27136 DevLoss: 0.27601\n",
            "\tEpoch 007/009: Loss at batchSetNum 50: 0.286977857 12.9 mins\n",
            "\t               WholeLoss: 0.28698 DevLoss: 0.2767\n",
            "\tEpoch 007/009: Loss at batchSetNum 60: 0.255779415 13.0 mins\n",
            "\t               WholeLoss: 0.25578 DevLoss: 0.27971\n",
            "\tEpoch 007/009: Loss at batchSetNum 70: 0.274542481 13.1 mins\n",
            "\t               WholeLoss: 0.27454 DevLoss: 0.2741\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.11556643\n",
            "Seq: 3 Size: 640 loss: 0.24276322\n",
            "Seq: 4 Size: 640 loss: 0.32841134\n",
            "Seq: 5 Size: 640 loss: 0.4110054\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.4536411\n",
            "Seq: 7 Size: 640 loss: 0.493336\n",
            "Seq: 8 Size: 640 loss: 0.5258018\n",
            "Seq: 9 Size: 640 loss: 0.5545919\n",
            "Seq: 10 Size: 640 loss: 0.5809121\n",
            "loss1: 0.274437\n",
            "loss2: 0.521657\n",
            "\tEpoch 007/009: Loss at batchSetNum 80: 0.264336526 13.2 mins\n",
            "\t               WholeLoss: 0.26434 DevLoss: 0.26808\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.10702288\n",
            "Seq: 3 Size: 640 loss: 0.23254742\n",
            "Seq: 4 Size: 640 loss: 0.32414725\n",
            "Seq: 5 Size: 640 loss: 0.41063768\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.45588923\n",
            "Seq: 7 Size: 640 loss: 0.49317655\n",
            "Seq: 8 Size: 640 loss: 0.5264631\n",
            "Seq: 9 Size: 640 loss: 0.55515283\n",
            "Seq: 10 Size: 640 loss: 0.58177626\n",
            "loss1: 0.268589\n",
            "loss2: 0.522492\n",
            "\tEpoch 007/009: Loss at batchSetNum 90: 0.246349767 13.3 mins\n",
            "\t               WholeLoss: 0.24635 DevLoss: 0.25207\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.09453733\n",
            "Seq: 3 Size: 640 loss: 0.21527745\n",
            "Seq: 4 Size: 640 loss: 0.31229877\n",
            "Seq: 5 Size: 640 loss: 0.4070583\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.45730227\n",
            "Seq: 7 Size: 640 loss: 0.5056002\n",
            "Seq: 8 Size: 640 loss: 0.5394498\n",
            "Seq: 9 Size: 640 loss: 0.56983596\n",
            "Seq: 10 Size: 640 loss: 0.5962508\n",
            "loss1: 0.257293\n",
            "loss2: 0.533688\n",
            "\tEpoch 007/009: Loss at batchSetNum 100: 0.241540670 13.4 mins\n",
            "\t               WholeLoss: 0.24154 DevLoss: 0.2452\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.09691201\n",
            "Seq: 3 Size: 640 loss: 0.21200562\n",
            "Seq: 4 Size: 640 loss: 0.30288202\n",
            "Seq: 5 Size: 640 loss: 0.4213008\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.46468994\n",
            "Seq: 7 Size: 640 loss: 0.50232476\n",
            "Seq: 8 Size: 640 loss: 0.5345201\n",
            "Seq: 9 Size: 640 loss: 0.56378156\n",
            "Seq: 10 Size: 640 loss: 0.5891348\n",
            "loss1: 0.258275\n",
            "loss2: 0.53089\n",
            "\tEpoch 007/009: Loss at batchSetNum 110: 0.239823818 13.5 mins\n",
            "\t               WholeLoss: 0.23982 DevLoss: 0.25464\n",
            "\tEpoch 007/009: Loss at batchSetNum 120: 0.237934455 13.6 mins\n",
            "\t               WholeLoss: 0.23793 DevLoss: 0.27059\n",
            "\tEpoch 007/009: Loss at batchSetNum 130: 0.237374753 13.7 mins\n",
            "\t               WholeLoss: 0.23737 DevLoss: 0.26732\n",
            "\tEpoch 007/009: Loss at batchSetNum 140: 0.250642836 13.8 mins\n",
            "\t               WholeLoss: 0.25064 DevLoss: 0.26847\n",
            "\tEpoch 007/009: Loss at batchSetNum 150: 0.231193751 13.9 mins\n",
            "\t               WholeLoss: 0.23119 DevLoss: 0.25729\n",
            "\tEpoch 007/009: Loss at batchSetNum 160: 0.231754854 14.0 mins\n",
            "\t               WholeLoss: 0.23175 DevLoss: 0.25909\n",
            "\tEpoch 007/009: Loss at batchSetNum 170: 0.232199311 14.0 mins\n",
            "\t               WholeLoss: 0.2322 DevLoss: 0.24613\n",
            "\tEpoch 007/009: Loss at batchSetNum 180: 0.255657852 14.1 mins\n",
            "\t               WholeLoss: 0.25566 DevLoss: 0.26148\n",
            "\tEpoch 007/009: Loss at batchSetNum 190: 0.234403774 14.2 mins\n",
            "\t               WholeLoss: 0.2344 DevLoss: 0.27074\n",
            "Epoch 007/009 completed \t - \tLoss at batchSetNum 199: 0.247765020 14.3 mins\n",
            "New Epoch Started: 8\n",
            "\tEpoch 008/009: Loss at batchSetNum 00: 0.236538485 14.3 mins\n",
            "\t               WholeLoss: 0.23654 DevLoss: 0.2364\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.077540025\n",
            "Seq: 3 Size: 640 loss: 0.20120017\n",
            "Seq: 4 Size: 640 loss: 0.29274487\n",
            "Seq: 5 Size: 640 loss: 0.396166\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.4761797\n",
            "Seq: 7 Size: 640 loss: 0.5130645\n",
            "Seq: 8 Size: 640 loss: 0.5437305\n",
            "Seq: 9 Size: 640 loss: 0.5715545\n",
            "Seq: 10 Size: 640 loss: 0.5974345\n",
            "loss1: 0.241913\n",
            "loss2: 0.540393\n",
            "\tEpoch 008/009: Loss at batchSetNum 10: 0.282002807 14.4 mins\n",
            "\t               WholeLoss: 0.282 DevLoss: 0.28585\n",
            "\tEpoch 008/009: Loss at batchSetNum 20: 0.257655025 14.5 mins\n",
            "\t               WholeLoss: 0.25766 DevLoss: 0.27511\n",
            "\tEpoch 008/009: Loss at batchSetNum 30: 0.231811792 14.6 mins\n",
            "\t               WholeLoss: 0.23181 DevLoss: 0.25487\n",
            "\tEpoch 008/009: Loss at batchSetNum 40: 0.239228934 14.7 mins\n",
            "\t               WholeLoss: 0.23923 DevLoss: 0.26017\n",
            "\tEpoch 008/009: Loss at batchSetNum 50: 0.254424870 14.8 mins\n",
            "\t               WholeLoss: 0.25442 DevLoss: 0.24188\n",
            "\tEpoch 008/009: Loss at batchSetNum 60: 0.222655803 14.8 mins\n",
            "\t               WholeLoss: 0.22266 DevLoss: 0.24959\n",
            "\tEpoch 008/009: Loss at batchSetNum 70: 0.225053310 14.9 mins\n",
            "\t               WholeLoss: 0.22505 DevLoss: 0.26215\n",
            "\tEpoch 008/009: Loss at batchSetNum 80: 0.270197988 15.0 mins\n",
            "\t               WholeLoss: 0.2702 DevLoss: 0.28574\n",
            "\tEpoch 008/009: Loss at batchSetNum 90: 0.255883753 15.1 mins\n",
            "\t               WholeLoss: 0.25588 DevLoss: 0.28338\n",
            "\tEpoch 008/009: Loss at batchSetNum 100: 0.246317118 15.2 mins\n",
            "\t               WholeLoss: 0.24632 DevLoss: 0.27795\n",
            "\tEpoch 008/009: Loss at batchSetNum 110: 0.233023092 15.3 mins\n",
            "\t               WholeLoss: 0.23302 DevLoss: 0.27875\n",
            "\tEpoch 008/009: Loss at batchSetNum 120: 0.216462165 15.4 mins\n",
            "\t               WholeLoss: 0.21646 DevLoss: 0.28221\n",
            "\tEpoch 008/009: Loss at batchSetNum 130: 0.272417873 15.4 mins\n",
            "\t               WholeLoss: 0.27242 DevLoss: 0.29963\n",
            "\tEpoch 008/009: Loss at batchSetNum 140: 0.254687458 15.5 mins\n",
            "\t               WholeLoss: 0.25469 DevLoss: 0.27014\n",
            "\tEpoch 008/009: Loss at batchSetNum 150: 0.222262919 15.6 mins\n",
            "\t               WholeLoss: 0.22226 DevLoss: 0.28039\n",
            "\tEpoch 008/009: Loss at batchSetNum 160: 0.226408899 15.7 mins\n",
            "\t               WholeLoss: 0.22641 DevLoss: 0.29107\n",
            "\tEpoch 008/009: Loss at batchSetNum 170: 0.217888474 15.8 mins\n",
            "\t               WholeLoss: 0.21789 DevLoss: 0.30115\n",
            "\tEpoch 008/009: Loss at batchSetNum 180: 0.215686664 15.9 mins\n",
            "\t               WholeLoss: 0.21569 DevLoss: 0.2946\n",
            "\tEpoch 008/009: Loss at batchSetNum 190: 0.206694186 16.0 mins\n",
            "\t               WholeLoss: 0.20669 DevLoss: 0.29136\n",
            "Epoch 008/009 completed \t - \tLoss at batchSetNum 199: 0.210564226 16.0 mins\n",
            "New Epoch Started: 9\n",
            "\tEpoch 009/009: Loss at batchSetNum 00: 0.204169020 16.0 mins\n",
            "\t               WholeLoss: 0.20417 DevLoss: 0.29433\n",
            "\tEpoch 009/009: Loss at batchSetNum 10: 0.221461415 16.2 mins\n",
            "\t               WholeLoss: 0.22146 DevLoss: 0.30344\n",
            "\tEpoch 009/009: Loss at batchSetNum 20: 0.208692074 16.2 mins\n",
            "\t               WholeLoss: 0.20869 DevLoss: 0.28414\n",
            "\tEpoch 009/009: Loss at batchSetNum 30: 0.199884072 16.3 mins\n",
            "\t               WholeLoss: 0.19988 DevLoss: 0.27962\n",
            "\tEpoch 009/009: Loss at batchSetNum 40: 0.199581355 16.4 mins\n",
            "\t               WholeLoss: 0.19958 DevLoss: 0.29876\n",
            "\tEpoch 009/009: Loss at batchSetNum 50: 0.216549486 16.5 mins\n",
            "\t               WholeLoss: 0.21655 DevLoss: 0.29187\n",
            "\tEpoch 009/009: Loss at batchSetNum 60: 0.195960402 16.6 mins\n",
            "\t               WholeLoss: 0.19596 DevLoss: 0.28476\n",
            "\tEpoch 009/009: Loss at batchSetNum 70: 0.190103143 16.7 mins\n",
            "\t               WholeLoss: 0.1901 DevLoss: 0.28157\n",
            "\tEpoch 009/009: Loss at batchSetNum 80: 0.206232592 16.8 mins\n",
            "\t               WholeLoss: 0.20623 DevLoss: 0.30428\n",
            "\tEpoch 009/009: Loss at batchSetNum 90: 0.194677413 16.9 mins\n",
            "\t               WholeLoss: 0.19468 DevLoss: 0.28225\n",
            "\tEpoch 009/009: Loss at batchSetNum 100: 0.220473126 17.0 mins\n",
            "\t               WholeLoss: 0.22047 DevLoss: 0.28737\n",
            "\tEpoch 009/009: Loss at batchSetNum 110: 0.186627835 17.0 mins\n",
            "\t               WholeLoss: 0.18663 DevLoss: 0.29531\n",
            "Stoppping due to overfit\n",
            "Epoch 009/009 completed \t - \tLoss at batchSetNum 110: 0.186627835 17.0 mins\n",
            "Final loss: 0.186627835 17.0 mins\n",
            "total epochs run so far(starts from 0): 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27wEJNmqO4yR",
        "colab_type": "code",
        "outputId": "b6f2ba57-d2d0-411f-e7aa-0f25837b532b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print('Best Dev Loss:',bestdevloss.numpy())\n",
        "print('Corresponding Best Train Loss:',bestwholeloss.numpy())\n",
        "print('Corresponding Best epoch:',bestepoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Dev Loss: 0.23639691\n",
            "Corresponding Best Train Loss: 0.23653848\n",
            "Corresponding Best epoch: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0GCUvlzuXxya"
      },
      "source": [
        "### Loss plot over the entire training sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "miigFwpMW_lO",
        "outputId": "0896d0e8-a2e3-4093-8c7e-fa29cbedf9de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        }
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.title(\"Loss vs. Iterations\")\n",
        "plt.ylabel('Cross Entropy Loss')\n",
        "plt.xlabel('Batch')\n",
        "plt.plot(loss_history,label='Sequence Batch Train Loss',color='grey',alpha=0.3)\n",
        "plt.plot(wholeLoss_history,label='Avg. Train Loss',color='blue',alpha=0.5)\n",
        "plt.plot(devLoss_history,label='Dev Loss',color='orange')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd61086a630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAJhCAYAAAD8C+z7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3D8wqL9Rrp1O"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gj8KZRWIrp1O",
        "outputId": "f3b06cbb-be7a-4197-a6bc-6aaafd1c5899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# print 1\n",
        "aloss=0\n",
        "aloss1=0\n",
        "aloss2=0\n",
        "\n",
        "print(\"SUMMARY\")\n",
        "for seqLen in range(minSeqSize,maxSeqSize+1):\n",
        "  X,Y=makeData(seqLen,seqLen,testBatchSize,testNumBatches)\n",
        "  _,_,interdevLoss=runAndGetLoss(X[seqLen],Y[seqLen],fwpn)\n",
        "  aloss=interdevLoss\n",
        "  print('Seq:',seqLen,'Size:',X[seqLen].shape[0],'loss:',aloss.numpy())\n",
        "  aloss1+=aloss\n",
        "\n",
        "print('---')\n",
        "\n",
        "for seqLen in range(maxSeqSize+1,maxSeqSize+1+5):\n",
        "  X,Y=makeData(seqLen,seqLen,testBatchSize,testNumBatches)\n",
        "  _,_,interdevLoss=runAndGetLoss(X[seqLen],Y[seqLen],fwpn)\n",
        "  aloss=interdevLoss\n",
        "  print('Seq:',seqLen,'Size:',X[seqLen].shape[0],'loss:',aloss.numpy())\n",
        "  aloss2+=aloss\n",
        "\n",
        "print('loss1:',np.round(aloss1.numpy()/float(maxSeqSize-minSeqSize+1),6))\n",
        "print('loss2:',np.round(aloss2.numpy()/float(5),6))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SUMMARY\n",
            "Seq: 2 Size: 640 loss: 0.051954843\n",
            "Seq: 3 Size: 640 loss: 0.15676808\n",
            "Seq: 4 Size: 640 loss: 0.3844819\n",
            "Seq: 5 Size: 640 loss: 0.5566648\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 0.59395415\n",
            "Seq: 7 Size: 640 loss: 0.6234234\n",
            "Seq: 8 Size: 640 loss: 0.6548155\n",
            "Seq: 9 Size: 640 loss: 0.6718853\n",
            "Seq: 10 Size: 640 loss: 0.69363725\n",
            "loss1: 0.287467\n",
            "loss2: 0.647543\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBX33JKWKgpm",
        "colab_type": "code",
        "outputId": "9862a909-faef-4f9b-dfaa-a3e3e4b88aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6885
        }
      },
      "source": [
        "# print 2\n",
        "print(\"DETAILED\")\n",
        "for seqLen in range(minSeqSize,maxSeqSize+1+5):\n",
        "  print()\n",
        "  if seqLen==maxSeqSize+1:\n",
        "      print('*********************************************************************************************************************************************')\n",
        "  print()\n",
        "  X,Y=makeData(seqLen,seqLen,testBatchSize,testNumBatches)\n",
        "  print('Seq:',seqLen,'Size:',X[seqLen].shape[0])\n",
        "  print('--------------------------------------------------------------------------------------------------------------------------------------------')\n",
        "  _,_,interdevLoss=runAndGetLoss(X[seqLen],Y[seqLen],fwpn,1,5)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DETAILED\n",
            "\n",
            "\n",
            "Seq: 2 Size: 640\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "====== Actual : [0.83574224 0.06025785]\n",
            "  - Predicted : [0.06025785 0.83574224]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.01662471 0.79840994]\n",
            "  - Predicted : [0.01662471 0.79840994]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.01190267 0.30980283]\n",
            "  - Predicted : [0.01190267 0.30980283]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.22009102 0.21262218]\n",
            "  - Predicted : [0.21262218 0.21262218]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.218951   0.90957433]\n",
            "  - Predicted : [0.218951   0.90957433]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.7722809  0.23816308]\n",
            "  - Predicted : [0.23816308 0.7722809 ]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.11175715 0.17111683]\n",
            "  - Predicted : [0.11175715 0.17111683]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Seq: 3 Size: 640\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "====== Actual : [0.08056203 0.5001086  0.61318433]\n",
            "  - Predicted : [0.08056203 0.61318433 0.61318433]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.2329092  0.07020402 0.18738341]\n",
            "  - Predicted : [0.18738341 0.18738341 0.18738341]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.4069094 0.5188632 0.8717007]\n",
            "  - Predicted : [0.4069094 0.8717007 0.8717007]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.6753187 0.8051869 0.6751493]\n",
            "  - Predicted : [0.6751493 0.8051869 0.8051869]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.91144454 0.20648947 0.9400306 ]\n",
            "  - Predicted : [0.20648947 0.9400306  0.9400306 ]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.48572576 0.30855882 0.5321798 ]\n",
            "  - Predicted : [0.30855882 0.48572576 0.48572576]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.7707616  0.89846134 0.7582664 ]\n",
            "  - Predicted : [0.7582664  0.89846134 0.89846134]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Seq: 4 Size: 640\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "====== Actual : [0.3436793  0.8430421  0.53148943 0.25742608]\n",
            "  - Predicted : [0.8430421 0.8430421 0.8430421 0.8430421]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.40579572 0.30376327 0.5133251  0.13184166]\n",
            "  - Predicted : [0.40579572 0.5133251  0.5133251  0.5133251 ]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.5592696  0.23702669 0.81275094 0.7276923 ]\n",
            "  - Predicted : [0.5592696  0.81275094 0.81275094 0.81275094]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.779533   0.21251976 0.59430885 0.06857959]\n",
            "  - Predicted : [0.779533 0.779533 0.779533 0.779533]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.02779458 0.7552295  0.851328   0.8428934 ]\n",
            "  - Predicted : [0.8428934 0.8428934 0.8428934 0.8428934]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.2759262  0.1008843  0.1489741  0.75869375]\n",
            "  - Predicted : [0.75869375 0.75869375 0.75869375 0.75869375]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.50838286 0.3371125  0.17839302 0.70552206]\n",
            "  - Predicted : [0.70552206 0.70552206 0.70552206 0.70552206]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Seq: 5 Size: 640\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "====== Actual : [0.19121641 0.78733873 0.5791036  0.6087658  0.5235984 ]\n",
            "  - Predicted : [0.78733873 0.78733873 0.78733873 0.78733873 0.78733873]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.04098156 0.25295863 0.40574715 0.11936255 0.24944665]\n",
            "  - Predicted : [0.24944665 0.24944665 0.24944665 0.24944665 0.24944665]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.68857193 0.04417931 0.47590184 0.8423354  0.4900378 ]\n",
            "  - Predicted : [0.68857193 0.68857193 0.68857193 0.68857193 0.68857193]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.8875878  0.7959116  0.72435594 0.14763933 0.20233624]\n",
            "  - Predicted : [0.8875878 0.8875878 0.8875878 0.8875878 0.8875878]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.9426623  0.2010902  0.89722365 0.8383759  0.8221969 ]\n",
            "  - Predicted : [0.89722365 0.89722365 0.89722365 0.89722365 0.89722365]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.47585344 0.28086522 0.33178705 0.0401283  0.339505  ]\n",
            "  - Predicted : [0.47585344 0.47585344 0.47585344 0.47585344 0.47585344]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.9741117  0.8869172  0.20360853 0.44996703 0.47063684]\n",
            "  - Predicted : [0.8869172 0.8869172 0.8869172 0.8869172 0.8869172]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*********************************************************************************************************************************************\n",
            "\n",
            "Seq: 6 Size: 640\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "====== Actual : [0.23905167 0.7045054  0.5915068  0.31807947 0.5892085  0.24077944]\n",
            "  - Predicted : [0.7045054 0.7045054 0.7045054 0.7045054 0.7045054 0.7045054]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.67876554 0.2884663  0.50486326 0.27318403 0.88227844 0.45606688]\n",
            "  - Predicted : [0.67876554 0.67876554 0.67876554 0.67876554 0.67876554 0.67876554]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.7871742  0.92541605 0.2786703  0.87503415 0.45287594 0.41084427]\n",
            "  - Predicted : [0.87503415 0.87503415 0.87503415 0.87503415 0.87503415 0.87503415]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.3879703  0.8902021  0.4201714  0.30113155 0.03651964 0.57013345]\n",
            "  - Predicted : [0.8902021 0.8902021 0.8902021 0.8902021 0.8902021 0.8902021]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.0741042  0.4742375  0.7314827  0.09193847 0.9896385  0.07469828]\n",
            "  - Predicted : [0.9896385 0.9896385 0.9896385 0.9896385 0.9896385 0.9896385]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.299875   0.7827368  0.6135532  0.04129643 0.7820833  0.866257  ]\n",
            "  - Predicted : [0.7827368 0.7827368 0.7827368 0.7827368 0.7827368 0.7827368]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.8886346  0.69253916 0.1013988  0.54558504 0.5729673  0.77934456]\n",
            "  - Predicted : [0.8886346 0.8886346 0.8886346 0.8886346 0.8886346 0.8886346]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Seq: 7 Size: 640\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "====== Actual : [0.5161091  0.9713692  0.13793504 0.11174459 0.8096141  0.2170962\n",
            " 0.8394443 ]\n",
            "  - Predicted : [0.9713692 0.9713692 0.9713692 0.9713692 0.9713692 0.9713692 0.9713692]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.5401534  0.84815294 0.4765258  0.4805786  0.0240792  0.86913085\n",
            " 0.5450281 ]\n",
            "  - Predicted : [0.84815294 0.84815294 0.84815294 0.84815294 0.84815294 0.84815294\n",
            " 0.84815294]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.478105   0.3984758  0.14486963 0.8508852  0.9457757  0.56955725\n",
            " 0.69759864]\n",
            "  - Predicted : [0.8508852 0.8508852 0.8508852 0.8508852 0.8508852 0.8508852 0.8508852]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.28206274 0.20224683 0.9524502  0.28905284 0.09395932 0.65713984\n",
            " 0.902575  ]\n",
            "  - Predicted : [0.9524502 0.9524502 0.9524502 0.9524502 0.9524502 0.9524502 0.9524502]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.87163323 0.47270432 0.32421938 0.04359481 0.16569069 0.73913664\n",
            " 0.61410666]\n",
            "  - Predicted : [0.87163323 0.87163323 0.87163323 0.87163323 0.87163323 0.87163323\n",
            " 0.87163323]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.8742586  0.31182075 0.28348154 0.24779154 0.06671028 0.17633589\n",
            " 0.82326657]\n",
            "  - Predicted : [0.8742586 0.8742586 0.8742586 0.8742586 0.8742586 0.8742586 0.8742586]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.7521191  0.41932195 0.9574202  0.08858002 0.21164545 0.8388485\n",
            " 0.43157515]\n",
            "  - Predicted : [0.9574202 0.9574202 0.9574202 0.9574202 0.9574202 0.9574202 0.9574202]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Seq: 8 Size: 640\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "====== Actual : [0.4077637  0.9377768  0.45307797 0.4291926  0.83193296 0.74982387\n",
            " 0.50805926 0.14766313]\n",
            "  - Predicted : [0.9377768 0.9377768 0.9377768 0.9377768 0.9377768 0.9377768 0.9377768\n",
            " 0.9377768]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.43094224 0.47781137 0.11661025 0.12834954 0.49374256 0.20290287\n",
            " 0.49014708 0.1973951 ]\n",
            "  - Predicted : [0.43094224 0.43094224 0.43094224 0.43094224 0.43094224 0.43094224\n",
            " 0.43094224 0.43094224]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.2135659  0.6157902  0.8725898  0.24167855 0.8941182  0.13335237\n",
            " 0.46902376 0.8844212 ]\n",
            "  - Predicted : [0.8725898 0.8725898 0.8725898 0.8725898 0.8725898 0.8725898 0.8725898\n",
            " 0.8725898]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.592948   0.43445432 0.5460099  0.99712247 0.5464191  0.6447808\n",
            " 0.4990743  0.6280711 ]\n",
            "  - Predicted : [0.99712247 0.99712247 0.99712247 0.99712247 0.99712247 0.99712247\n",
            " 0.99712247 0.99712247]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.4333413  0.50515014 0.15238862 0.6350074  0.39611685 0.11515445\n",
            " 0.48737702 0.3231017 ]\n",
            "  - Predicted : [0.6350074 0.6350074 0.6350074 0.6350074 0.6350074 0.6350074 0.6350074\n",
            " 0.6350074]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.05634461 0.44911757 0.6363658  0.93763113 0.02532229 0.3544204\n",
            " 0.0940268  0.35939685]\n",
            "  - Predicted : [0.35939685 0.35939685 0.35939685 0.35939685 0.35939685 0.35939685\n",
            " 0.35939685 0.35939685]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.1274517  0.8448477  0.17881829 0.60717756 0.9382175  0.9032744\n",
            " 0.18648298 0.11095741]\n",
            "  - Predicted : [0.9382175 0.9382175 0.9382175 0.9382175 0.9382175 0.9382175 0.9382175\n",
            " 0.9382175]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Seq: 9 Size: 640\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "====== Actual : [0.60583377 0.86665726 0.93833435 0.0227743  0.5064685  0.47814965\n",
            " 0.7741509  0.68737215 0.47843754]\n",
            "  - Predicted : [0.86665726 0.86665726 0.86665726 0.86665726 0.86665726 0.86665726\n",
            " 0.86665726 0.86665726 0.86665726]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.72282726 0.7553067  0.4322209  0.6350735  0.37621257 0.92069465\n",
            " 0.19476072 0.07351932 0.51381457]\n",
            "  - Predicted : [0.92069465 0.92069465 0.92069465 0.92069465 0.92069465 0.92069465\n",
            " 0.92069465 0.92069465 0.92069465]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.84222704 0.00350055 0.5700958  0.466018   0.40397677 0.5418098\n",
            " 0.79646015 0.3210686  0.43373144]\n",
            "  - Predicted : [0.84222704 0.84222704 0.84222704 0.84222704 0.84222704 0.84222704\n",
            " 0.84222704 0.84222704 0.84222704]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.11201692 0.96754605 0.88400775 0.08960901 0.5999067  0.5337977\n",
            " 0.8641564  0.9830748  0.24421136]\n",
            "  - Predicted : [0.88400775 0.88400775 0.88400775 0.88400775 0.88400775 0.88400775\n",
            " 0.88400775 0.88400775 0.88400775]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.92610985 0.04928457 0.16308558 0.9744515  0.28945848 0.9042387\n",
            " 0.6680615  0.59051865 0.66437316]\n",
            "  - Predicted : [0.92610985 0.9744515  0.9744515  0.9744515  0.9744515  0.9744515\n",
            " 0.9744515  0.9744515  0.9744515 ]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.31501806 0.9944818  0.46190873 0.1276752  0.17110051 0.7046664\n",
            " 0.34457663 0.5326619  0.3914566 ]\n",
            "  - Predicted : [0.9944818 0.9944818 0.9944818 0.9944818 0.9944818 0.9944818 0.9944818\n",
            " 0.9944818 0.9944818]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.3398793  0.43450528 0.38599375 0.81556517 0.84939605 0.6248873\n",
            " 0.07126666 0.88245314 0.84438133]\n",
            "  - Predicted : [0.81556517 0.81556517 0.81556517 0.81556517 0.81556517 0.81556517\n",
            " 0.81556517 0.81556517 0.81556517]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Seq: 10 Size: 640\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "====== Actual : [0.22310425 0.21548238 0.16671984 0.09234142 0.8184715  0.55958635\n",
            " 0.17878523 0.18940555 0.8239627  0.5206049 ]\n",
            "  - Predicted : [0.8184715 0.8184715 0.8184715 0.8184715 0.8184715 0.8184715 0.8184715\n",
            " 0.8184715 0.8184715 0.8184715]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.64108694 0.9215544  0.5496397  0.7022597  0.5335491  0.80902964\n",
            " 0.3984941  0.59239954 0.7900002  0.2362623 ]\n",
            "  - Predicted : [0.9215544 0.9215544 0.9215544 0.9215544 0.9215544 0.9215544 0.9215544\n",
            " 0.9215544 0.9215544 0.9215544]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.40630975 0.6643387  0.4306792  0.9832877  0.57199126 0.6302086\n",
            " 0.5097655  0.17489414 0.41207007 0.7488996 ]\n",
            "  - Predicted : [0.9832877 0.9832877 0.9832877 0.9832877 0.9832877 0.9832877 0.9832877\n",
            " 0.9832877 0.9832877 0.9832877]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.8080539  0.00361666 0.20645495 0.15724239 0.69259495 0.7878767\n",
            " 0.35050583 0.8170975  0.24136421 0.08478203]\n",
            "  - Predicted : [0.8080539 0.8080539 0.8080539 0.8080539 0.8080539 0.8080539 0.8080539\n",
            " 0.8080539 0.8080539 0.8080539]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.94780564 0.56708336 0.97635394 0.5631446  0.607248   0.82059205\n",
            " 0.9744998  0.8970849  0.71141917 0.19944397]\n",
            "  - Predicted : [0.94780564 0.94780564 0.94780564 0.94780564 0.94780564 0.94780564\n",
            " 0.94780564 0.94780564 0.94780564 0.94780564]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.04186352 0.83317035 0.64542884 0.32958835 0.42090645 0.35379264\n",
            " 0.07806554 0.7739282  0.73616165 0.08550877]\n",
            "  - Predicted : [0.64542884 0.64542884 0.64542884 0.64542884 0.64542884 0.64542884\n",
            " 0.64542884 0.64542884 0.64542884 0.64542884]\n",
            "\n",
            "\n",
            "\n",
            "====== Actual : [0.64142936 0.61101454 0.6438014  0.48224285 0.8567375  0.9645043\n",
            " 0.7620893  0.7042966  0.9744176  0.2994277 ]\n",
            "  - Predicted : [0.64142936 0.64142936 0.64142936 0.64142936 0.64142936 0.64142936\n",
            " 0.64142936 0.64142936 0.64142936 0.64142936]\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxQnlRZHFf3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}