{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_based_pointer_net.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msapurva/Sorting-arrays-using-Pointer_Networks_w_Fast_Weights/blob/master/LSTM_based_pointer_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zWFaAnmetOTr",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime as dt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zyK1oRMrp0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.enable_eager_execution()\n",
        "np.random.seed(42)\n",
        "tf.random.set_random_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH5gPEjcrp0n",
        "colab_type": "text"
      },
      "source": [
        "# Experiment Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWdDXmrXrp0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training and Dev Data\n",
        "minSeqSize=2\n",
        "maxSeqSize=5\n",
        "\n",
        "# Training Data \n",
        "batchSize=32\n",
        "numOfBatches=200\n",
        "datasize=batchSize*numOfBatches*(maxSeqSize-minSeqSize+1)\n",
        "\n",
        "# Dev data\n",
        "devBatchSize=64\n",
        "devNumBatches=10\n",
        "\n",
        "# Test data\n",
        "testBatchSize=64\n",
        "testNumBatches=10\n",
        "\n",
        "# Ptr Model Config\n",
        "hidden_dimensions=300 \n",
        "\n",
        "# over-fitting tolerance cut-off\n",
        "# for some cases the initial batch or an intermediate batch might cross this due to random chance and not due to overfitting. \n",
        "#In such cases modify this and run the code again\n",
        "overfitcutoff=0.1 \n",
        "\n",
        "\n",
        "# epoch configuration defined later below. Look at 'epochs' variable to change this\n",
        "\n",
        "\n",
        "# For changing the start symbol representation, update the line with comment '# start symbol append' in makeData function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wmYMjSIyL2F8"
      },
      "source": [
        "### Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS-oQ9TQrp0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def makeData(minSeqSize,maxSeqSize,batchSize,numOfBatches):\n",
        "    X={}\n",
        "    Y={}\n",
        "    datasize=batchSize*numOfBatches*(maxSeqSize-minSeqSize+1)\n",
        "    \n",
        "    for seqLen in range(minSeqSize,maxSeqSize+1):\n",
        "        X[seqLen]=[]\n",
        "        Y[seqLen]=[]\n",
        "\n",
        "        for dataidx in range(int(datasize/(maxSeqSize-minSeqSize+1))):\n",
        "            seqBase=np.random.uniform(size=(1,seqLen-1))\n",
        "            aSeq=np.concatenate((np.zeros((1,seqLen-1),dtype=np.float32),seqBase),axis=0)\n",
        "            aSeq=aSeq.T\n",
        "            aSeq=np.concatenate((np.array([[1,0]],dtype=np.float32),aSeq),axis=0) # start symbol append\n",
        "            X[seqLen]+=[aSeq]\n",
        "            aRec=[]\n",
        "            for e in np.sort(seqBase[0]):\n",
        "                idx=list(seqBase[0]).index(e)\n",
        "                aRec+=[np.zeros(seqLen,dtype=np.float32)]\n",
        "                aRec[-1][idx+1]=1\n",
        "            aRec+=[np.zeros(seqLen,dtype=np.float32)]\n",
        "            aRec[-1][0]=1\n",
        "            Y[seqLen]+=[aRec]\n",
        "\n",
        "        X[seqLen]=np.array(X[seqLen],dtype=np.float32)\n",
        "        Y[seqLen]=np.array(Y[seqLen],dtype=np.float32)\n",
        "    return X,Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuO8Af9grp0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainX,trainY=makeData(minSeqSize,maxSeqSize,batchSize,numOfBatches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsYIS0Terp0t",
        "colab_type": "code",
        "outputId": "2af307a8-937a-4c57-ade7-9ba44570bbe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Train X for',minSeqSize,':',trainX[minSeqSize].shape)\n",
        "print('Train Y for',minSeqSize,':',trainY[minSeqSize].shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train X for 2 : (6400, 2, 2)\n",
            "Train Y for 2 : (6400, 2, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "97GlpqgvL8PK"
      },
      "source": [
        "## Encoder & Decoder Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uqAZrGBMTGDj",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, hidden_dimensions):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm = layers.CuDNNLSTM(hidden_dimensions, return_sequences=True, return_state=True)\n",
        "        \n",
        "    def call(self, x):\n",
        "        output, state_h, state_c  = self.lstm(x)        \n",
        "        return output, [state_h, state_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tLNzTuJfWPhk",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "     def __init__(self, hidden_dimensions):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.lstm = layers.CuDNNLSTM(hidden_dimensions, return_sequences=True, return_state=True)\n",
        "     \n",
        "     def call(self, x, hidden_states):\n",
        "        dec_output, state_h, state_c  = self.lstm(x, initial_state=hidden_states)    \n",
        "        return dec_output, [state_h, state_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eTSf9SdkBjcM",
        "colab": {}
      },
      "source": [
        "class Attention(tf.keras.Model):\n",
        "     def __init__(self, hidden_dimensions):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(hidden_dimensions, use_bias=False)\n",
        "        self.W2 = tf.keras.layers.Dense(hidden_dimensions, use_bias=False)\n",
        "        self.V = tf.keras.layers.Dense(1, use_bias=False)\n",
        "        \n",
        "     \n",
        "     def call(self, encoder_outputs, dec_output):\n",
        "        w1_e = self.W1(encoder_outputs)\n",
        "        w2_d = self.W2(dec_output)\n",
        "        tanh_output = tf.nn.tanh(w1_e + w2_d)\n",
        "        v_dot_tanh = self.V(tanh_output)\n",
        "        attention_weights = tf.nn.softmax(v_dot_tanh, axis=1)\n",
        "        return tf.reshape(attention_weights, (attention_weights.shape[0], attention_weights.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w_shAefxqfVs"
      },
      "source": [
        "### Initialize encoder and decoder network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xLlEdEXmVFGq",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(hidden_dimensions)\n",
        "decoder = Decoder(hidden_dimensions)\n",
        "attention = Attention(hidden_dimensions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiwTmjWWrp02",
        "colab_type": "text"
      },
      "source": [
        "### Batch Infer Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0C6_Sz_rp03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def infer(encoder,decoder,attention,X,Y,verbose=0):\n",
        "    loss=0\n",
        "    \n",
        "    if verbose==2:\n",
        "        print('X:')\n",
        "        print(X)\n",
        "        print('Y:')\n",
        "        print(Y)\n",
        "\n",
        "    #Initialize values\n",
        "    attention_vector_array = []\n",
        "    final_output_sequence = []\n",
        "\n",
        "    encoder_input, target_data=X,Y\n",
        "    encoder_outputs, encoder_states = encoder(encoder_input)\n",
        "   \n",
        "    dec_input = tf.convert_to_tensor(np.array([[[0,0]]]*X.shape[0],dtype=np.float32)) # prepare initial decoder input    \n",
        "    decoder_states = encoder_states # loading the final encoder states to decoder network as initial hidden states\n",
        "    if verbose==2:\n",
        "        print('\\n\\nPREDICTIONS')\n",
        "        print('-----------')\n",
        "    \n",
        "    result=[]\n",
        "    for i in range(0, encoder_input.shape[1]):\n",
        "        result+=[[0]*encoder_input.shape[0]]\n",
        "        decoder_output, decoder_states = decoder(dec_input, decoder_states)\n",
        "        target_prediction = attention(encoder_outputs, decoder_output)\n",
        "        ei_slice_stack=[]\n",
        "        tarCounter=0\n",
        "        for tar_data_slice in target_prediction:\n",
        "            ei_slice_stack+=[encoder_input[tarCounter,np.argmax(tar_data_slice)]]\n",
        "            tarCounter+=1\n",
        "        ei_slice_stack=tf.convert_to_tensor(np.array(ei_slice_stack,dtype=np.float32))\n",
        "\n",
        "        if verbose==2:\n",
        "            outputCounter=0\n",
        "            print('\\nFor position:',i)\n",
        "            for x in X:\n",
        "                print(\"\\tInput %d, Predicted:%d->%s\"%(outputCounter,np.argmax(target_prediction[outputCounter]), str(x[np.argmax(target_prediction[outputCounter])])),\n",
        "                     '\\tExpected:%d->%s:'%(np.argmax(target_data[outputCounter,i]),encoder_input[outputCounter,np.argmax(target_data[outputCounter,i])]))\n",
        "                outputCounter+=1\n",
        "        if verbose==1:\n",
        "            outputCounter=0\n",
        "            for x in X:\n",
        "                result[-1][outputCounter]=encoder_input[outputCounter,np.argmax(target_prediction[outputCounter])]\n",
        "                outputCounter+=1\n",
        "\n",
        "\n",
        "        loss += tf.reduce_mean(tf.keras.backend.categorical_crossentropy(target_data[:,i], target_prediction))\n",
        "        \n",
        "        dec_input = tf.expand_dims(ei_slice_stack, 1)\n",
        "\n",
        "    if verbose==1:\n",
        "        outputCounter=-1\n",
        "        for x in X:\n",
        "            outputCounter+=1\n",
        "            print('\\n===== PREDICTED:',outputCounter,'=====\\n')\n",
        "            for i in range(0, encoder_input.shape[1]):\n",
        "                print('\\t-',result[i][outputCounter])\n",
        "            print('\\n--- ACTUAL:',outputCounter,'---\\n')\n",
        "            for i in range(0, encoder_input.shape[1]):\n",
        "                print('\\t-',encoder_input[outputCounter][np.argmax(target_data[outputCounter][i])])\n",
        "\n",
        "            \n",
        "    return loss.numpy()/(X.shape[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LkM2Lp_zq3VK"
      },
      "source": [
        "#### Output of network before training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "TRc1TdSrrp05",
        "colab_type": "code",
        "outputId": "d3ee9021-33d1-41b8-98ca-aa588bf468a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "# Use data point\n",
        "seqLen=minSeqSize\n",
        "infer(encoder,decoder,attention,trainX[seqLen][0:2],trainY[seqLen][0:2],1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:642: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "===== PREDICTED: 0 =====\n",
            "\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 0 ---\n",
            "\n",
            "\t- [0.         0.37454012]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 1 =====\n",
            "\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 1 ---\n",
            "\n",
            "\t- [0.        0.9507143]\n",
            "\t- [1. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6931508779525757"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "01MfSZZoMNwM"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dER3y_P7rp07",
        "colab_type": "text"
      },
      "source": [
        "#### Prep training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZegfAE5rp08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batchedDataset={}\n",
        "for seqLen in range(minSeqSize,maxSeqSize+1):\n",
        "    batchedDataset[seqLen]=[]\n",
        "    for aBatch in tf.data.Dataset.from_tensor_slices((trainX[seqLen],trainY[seqLen])).batch(batchSize):\n",
        "        batchedDataset[seqLen]+=[aBatch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5EutlgJrp0-",
        "colab_type": "text"
      },
      "source": [
        "#### Config and run training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA7UgeVErp0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lastepoch=0\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "loss_history = []\n",
        "wholeLoss_history=[]\n",
        "devLoss_history=[]\n",
        "total_attention = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYNF6TS2rp1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "overfitFlag=False\n",
        "tolWindow=5\n",
        "tol=0.5\n",
        "simtolWindow=10\n",
        "simtolFraction=0.01\n",
        "\n",
        "bestdevloss=1000\n",
        "bestwholeloss=1000\n",
        "bestepoch=-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa4s4GqAwGGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "devX,devY=makeData(minSeqSize,maxSeqSize,devBatchSize,devNumBatches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT5Eb-mbrp1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 5\n",
        "overfitFlag=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "HrfsigX4rp1F",
        "colab_type": "code",
        "outputId": "5a9bd4b1-10cc-47ee-d81f-f4783807dd03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4488
        }
      },
      "source": [
        "for epoch in range(lastepoch,epochs+lastepoch):\n",
        "    if overfitFlag:\n",
        "        break\n",
        "    print(\"NEW EPOCH:\",epoch+1)\n",
        "    lastepoch=epoch\n",
        "    for batch in range(numOfBatches):\n",
        "        wholeLoss=0\n",
        "        for seqLen in range(minSeqSize,maxSeqSize+1):\n",
        "            loss=0\n",
        "            with tf.GradientTape() as tape:            \n",
        "                encoder_input, target_data=batchedDataset[seqLen][batch]\n",
        "                # run the decoder\n",
        "                encoder_outputs, encoder_states = encoder(encoder_input)\n",
        "                dec_input = tf.convert_to_tensor(np.array([[[0,0]]]*batchSize,dtype=np.float32))\n",
        "                decoder_states = encoder_states\n",
        "                for i in range(encoder_input.shape[1]):\n",
        "                    decoder_output, decoder_states = decoder(dec_input, decoder_states)\n",
        "                    target_prediction = attention(encoder_outputs, decoder_output)\n",
        "                    tar_data = target_data[:, i]                \n",
        "                    tarCounter=0\n",
        "                    ei_slice_stack=[]\n",
        "                    for tar_data_slice in tar_data:\n",
        "                        ei_slice_stack+=[encoder_input[tarCounter,np.argmax(tar_data_slice)]]\n",
        "                        tarCounter+=1\n",
        "                    ei_slice_stack=tf.convert_to_tensor(np.array(ei_slice_stack,dtype=np.float32))\n",
        "                    dec_input = tf.expand_dims(ei_slice_stack, 1)\n",
        "                    loss += tf.reduce_mean(tf.keras.backend.categorical_crossentropy(tar_data, target_prediction))\n",
        "\n",
        "            # loss per output for a batch of same sized seq\n",
        "            batch_loss = (loss / (encoder_input.shape[1].value))\n",
        "            wholeLoss+=batch_loss\n",
        "            variables = encoder.variables + decoder.variables\n",
        "            grads = tape.gradient(loss, variables)\n",
        "            optimizer.apply_gradients(zip(grads, variables), global_step=tf.train.get_or_create_global_step())\n",
        "            loss_history.append(batch_loss.numpy())\n",
        "\n",
        "        wholeLoss=(wholeLoss/(maxSeqSize-minSeqSize+1))\n",
        "        \n",
        "        for _ in range(maxSeqSize-minSeqSize+1):\n",
        "          wholeLoss_history.append(wholeLoss.numpy())\n",
        "        \n",
        "        if batch % 10 == 0:\n",
        "            print(\"\\tEpoch {:03d}/{:03d}: Loss at step {:02d}: {:.9f}\".format((epoch+1), epochs, batch, tf.reduce_mean(tf.keras.backend.categorical_crossentropy(tar_data, target_prediction))),dt.now())\n",
        "            devLoss=0\n",
        "            for seqLen in range(minSeqSize,maxSeqSize+1):\n",
        "\n",
        "                devLoss+=infer(encoder,decoder,attention,devX[seqLen],devY[seqLen])\n",
        "            devLoss=(devLoss/(maxSeqSize-minSeqSize+1))\n",
        "            for _ in range((maxSeqSize-minSeqSize+1)*10):\n",
        "              devLoss_history.append(devLoss)\n",
        "            \n",
        "            if len(devLoss_history)==0:\n",
        "              bestdevloss=devLoss\n",
        "              bestwholeloss=wholeLoss\n",
        "              bestepoch=lastepoch\n",
        "            \n",
        "            if bestdevloss>min(devLoss_history):\n",
        "              print(\" ----------------------------------------------------------------- new best found ^.^\")\n",
        "              bestdevloss=devLoss\n",
        "              bestwholeloss=wholeLoss\n",
        "              bestepoch=lastepoch             \n",
        "           \n",
        "            if -wholeLoss+devLoss>overfitcutoff:\n",
        "                print('Dev loss too different than Train loss. Stopping Training')\n",
        "                overfitFlag=True\n",
        "                break\n",
        "            print('\\t               WholeLoss:',np.round(wholeLoss.numpy(),5),'DevLoss:',np.round(devLoss,5))\n",
        "    print(\"Epoch {:03d}/{:03d} completed \\t - \\tBatch loss: {:.9f}\".format((epoch+1), epochs, tf.reduce_mean(tf.keras.backend.categorical_crossentropy(tar_data, target_prediction))),dt.now())\n",
        "print(\"Final loss: {:.9f}\".format(tf.reduce_mean(tf.keras.backend.categorical_crossentropy(tar_data, target_prediction))),dt.now())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NEW EPOCH: 1\n",
            "\tEpoch 001/005: Loss at step 00: 1.601598144 2019-05-08 14:31:23.427137\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 1.19689 DevLoss: 1.19688\n",
            "\tEpoch 001/005: Loss at step 10: 1.252636075 2019-05-08 14:31:37.734177\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 1.14838 DevLoss: 1.13285\n",
            "\tEpoch 001/005: Loss at step 20: 0.514083862 2019-05-08 14:31:52.128439\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.94166 DevLoss: 0.91563\n",
            "\tEpoch 001/005: Loss at step 30: 0.213444620 2019-05-08 14:32:06.516137\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.7316 DevLoss: 0.71819\n",
            "\tEpoch 001/005: Loss at step 40: 0.092373803 2019-05-08 14:32:20.833437\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.65864 DevLoss: 0.65662\n",
            "\tEpoch 001/005: Loss at step 50: 0.057874013 2019-05-08 14:32:35.302695\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.63477 DevLoss: 0.63346\n",
            "\tEpoch 001/005: Loss at step 60: 0.044669691 2019-05-08 14:32:49.819229\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.62355 DevLoss: 0.6227\n",
            "\tEpoch 001/005: Loss at step 70: 0.036245067 2019-05-08 14:33:04.247547\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.61465 DevLoss: 0.61445\n",
            "\tEpoch 001/005: Loss at step 80: 0.031189829 2019-05-08 14:33:18.662597\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.60684 DevLoss: 0.60737\n",
            "\tEpoch 001/005: Loss at step 90: 0.033277337 2019-05-08 14:33:33.057826\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.60106 DevLoss: 0.59628\n",
            "\tEpoch 001/005: Loss at step 100: 0.033346836 2019-05-08 14:33:47.527267\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.58933 DevLoss: 0.58518\n",
            "\tEpoch 001/005: Loss at step 110: 0.035401352 2019-05-08 14:34:01.909664\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.57632 DevLoss: 0.57353\n",
            "\tEpoch 001/005: Loss at step 120: 0.036147729 2019-05-08 14:34:16.315835\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.55565 DevLoss: 0.55947\n",
            "\tEpoch 001/005: Loss at step 130: 0.030941149 2019-05-08 14:34:30.794767\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.54629 DevLoss: 0.54434\n",
            "\tEpoch 001/005: Loss at step 140: 0.037137352 2019-05-08 14:34:45.642179\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.51735 DevLoss: 0.5279\n",
            "\tEpoch 001/005: Loss at step 150: 0.035467245 2019-05-08 14:35:00.341058\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.51645 DevLoss: 0.51362\n",
            "\tEpoch 001/005: Loss at step 160: 0.036707811 2019-05-08 14:35:14.828644\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.49962 DevLoss: 0.50281\n",
            "\tEpoch 001/005: Loss at step 170: 0.034735285 2019-05-08 14:35:29.345283\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.49267 DevLoss: 0.49082\n",
            "\tEpoch 001/005: Loss at step 180: 0.041320253 2019-05-08 14:35:43.684683\n",
            "\t               WholeLoss: 0.47991 DevLoss: 0.49093\n",
            "\tEpoch 001/005: Loss at step 190: 0.032856867 2019-05-08 14:35:57.999776\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.47462 DevLoss: 0.47784\n",
            "Epoch 001/005 completed \t - \tBatch loss: 0.039317112 2019-05-08 14:36:11.274109\n",
            "NEW EPOCH: 2\n",
            "\tEpoch 002/005: Loss at step 00: 0.032658108 2019-05-08 14:36:12.377822\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.45774 DevLoss: 0.46195\n",
            "\tEpoch 002/005: Loss at step 10: 0.036705505 2019-05-08 14:36:27.430247\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.44694 DevLoss: 0.45135\n",
            "\tEpoch 002/005: Loss at step 20: 0.033839066 2019-05-08 14:36:42.922587\n",
            "\t               WholeLoss: 0.44065 DevLoss: 0.45296\n",
            "\tEpoch 002/005: Loss at step 30: 0.036203850 2019-05-08 14:36:57.430567\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.45175 DevLoss: 0.43837\n",
            "\tEpoch 002/005: Loss at step 40: 0.037724655 2019-05-08 14:37:12.035076\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.43178 DevLoss: 0.43023\n",
            "\tEpoch 002/005: Loss at step 50: 0.042734534 2019-05-08 14:37:26.437051\n",
            "\t               WholeLoss: 0.44765 DevLoss: 0.43195\n",
            "\tEpoch 002/005: Loss at step 60: 0.039125949 2019-05-08 14:37:40.728971\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.41538 DevLoss: 0.41594\n",
            "\tEpoch 002/005: Loss at step 70: 0.041451305 2019-05-08 14:37:55.123087\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.40375 DevLoss: 0.39814\n",
            "\tEpoch 002/005: Loss at step 80: 0.043117195 2019-05-08 14:38:09.556897\n",
            "\t               WholeLoss: 0.3918 DevLoss: 0.40053\n",
            "\tEpoch 002/005: Loss at step 90: 0.038752377 2019-05-08 14:38:23.949386\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.38261 DevLoss: 0.38638\n",
            "\tEpoch 002/005: Loss at step 100: 0.035766330 2019-05-08 14:38:38.378594\n",
            "\t               WholeLoss: 0.40022 DevLoss: 0.39386\n",
            "\tEpoch 002/005: Loss at step 110: 0.037334070 2019-05-08 14:38:52.833114\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.36093 DevLoss: 0.36603\n",
            "\tEpoch 002/005: Loss at step 120: 0.038611233 2019-05-08 14:39:07.363368\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.39488 DevLoss: 0.36406\n",
            "\tEpoch 002/005: Loss at step 130: 0.033384196 2019-05-08 14:39:21.749072\n",
            "\t               WholeLoss: 0.36873 DevLoss: 0.36689\n",
            "\tEpoch 002/005: Loss at step 140: 0.039403267 2019-05-08 14:39:36.184620\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.33379 DevLoss: 0.347\n",
            "\tEpoch 002/005: Loss at step 150: 0.038700715 2019-05-08 14:39:50.752042\n",
            "\t               WholeLoss: 0.34774 DevLoss: 0.35647\n",
            "\tEpoch 002/005: Loss at step 160: 0.039821237 2019-05-08 14:40:05.572247\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.32667 DevLoss: 0.339\n",
            "\tEpoch 002/005: Loss at step 170: 0.035875700 2019-05-08 14:40:19.942073\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.34031 DevLoss: 0.33173\n",
            "\tEpoch 002/005: Loss at step 180: 0.038440324 2019-05-08 14:40:34.526081\n",
            "\t               WholeLoss: 0.33078 DevLoss: 0.3381\n",
            "\tEpoch 002/005: Loss at step 190: 0.041567184 2019-05-08 14:40:48.969350\n",
            "\t               WholeLoss: 0.34753 DevLoss: 0.33238\n",
            "Epoch 002/005 completed \t - \tBatch loss: 0.040934127 2019-05-08 14:41:02.278767\n",
            "NEW EPOCH: 3\n",
            "\tEpoch 003/005: Loss at step 00: 0.040335920 2019-05-08 14:41:03.378862\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.31993 DevLoss: 0.31183\n",
            "\tEpoch 003/005: Loss at step 10: 0.039033029 2019-05-08 14:41:17.729277\n",
            "\t               WholeLoss: 0.38341 DevLoss: 0.31883\n",
            "\tEpoch 003/005: Loss at step 20: 0.042638153 2019-05-08 14:41:32.253217\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.30568 DevLoss: 0.30406\n",
            "\tEpoch 003/005: Loss at step 30: 0.042848192 2019-05-08 14:41:48.201958\n",
            "\t               WholeLoss: 0.32506 DevLoss: 0.31603\n",
            "\tEpoch 003/005: Loss at step 40: 0.041854806 2019-05-08 14:42:02.541217\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.29627 DevLoss: 0.29509\n",
            "\tEpoch 003/005: Loss at step 50: 0.041940909 2019-05-08 14:42:17.023893\n",
            "\t               WholeLoss: 0.38674 DevLoss: 0.34237\n",
            "\tEpoch 003/005: Loss at step 60: 0.041537225 2019-05-08 14:42:31.503400\n",
            "\t               WholeLoss: 0.27717 DevLoss: 0.29823\n",
            "\tEpoch 003/005: Loss at step 70: 0.039803334 2019-05-08 14:42:45.960822\n",
            "\t               WholeLoss: 0.30747 DevLoss: 0.31313\n",
            "\tEpoch 003/005: Loss at step 80: 0.043409329 2019-05-08 14:43:00.382118\n",
            "\t               WholeLoss: 0.27694 DevLoss: 0.30141\n",
            "\tEpoch 003/005: Loss at step 90: 0.043271296 2019-05-08 14:43:14.852825\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.30332 DevLoss: 0.28037\n",
            "\tEpoch 003/005: Loss at step 100: 0.038863048 2019-05-08 14:43:29.272805\n",
            "\t               WholeLoss: 0.31303 DevLoss: 0.29347\n",
            "\tEpoch 003/005: Loss at step 110: 0.042800773 2019-05-08 14:43:43.672720\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.27675 DevLoss: 0.27291\n",
            "\tEpoch 003/005: Loss at step 120: 0.041599728 2019-05-08 14:43:58.060754\n",
            "\t               WholeLoss: 0.2746 DevLoss: 0.29186\n",
            "\tEpoch 003/005: Loss at step 130: 0.038560160 2019-05-08 14:44:12.558245\n",
            "\t               WholeLoss: 0.33442 DevLoss: 0.29383\n",
            "\tEpoch 003/005: Loss at step 140: 0.043945849 2019-05-08 14:44:27.003456\n",
            "\t               WholeLoss: 0.28683 DevLoss: 0.32071\n",
            "\tEpoch 003/005: Loss at step 150: 0.041844100 2019-05-08 14:44:41.382267\n",
            "\t               WholeLoss: 0.28287 DevLoss: 0.28225\n",
            "\tEpoch 003/005: Loss at step 160: 0.040461492 2019-05-08 14:44:55.705073\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.25409 DevLoss: 0.26766\n",
            "\tEpoch 003/005: Loss at step 170: 0.041540138 2019-05-08 14:45:10.834071\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.2637 DevLoss: 0.25725\n",
            "\tEpoch 003/005: Loss at step 180: 0.040483449 2019-05-08 14:45:25.224550\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.27837 DevLoss: 0.25507\n",
            "\tEpoch 003/005: Loss at step 190: 0.043856859 2019-05-08 14:45:39.647846\n",
            "\t               WholeLoss: 0.27305 DevLoss: 0.25562\n",
            "Epoch 003/005 completed \t - \tBatch loss: 0.041476712 2019-05-08 14:45:52.914082\n",
            "NEW EPOCH: 4\n",
            "\tEpoch 004/005: Loss at step 00: 0.040932313 2019-05-08 14:45:53.996650\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.25214 DevLoss: 0.25066\n",
            "\tEpoch 004/005: Loss at step 10: 0.039220504 2019-05-08 14:46:08.426996\n",
            "\t               WholeLoss: 0.26504 DevLoss: 0.2542\n",
            "\tEpoch 004/005: Loss at step 20: 0.041904043 2019-05-08 14:46:22.840111\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.26092 DevLoss: 0.24959\n",
            "\tEpoch 004/005: Loss at step 30: 0.041428152 2019-05-08 14:46:37.338383\n",
            "\t               WholeLoss: 0.27376 DevLoss: 0.26799\n",
            "\tEpoch 004/005: Loss at step 40: 0.038981967 2019-05-08 14:46:52.999454\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.24734 DevLoss: 0.24374\n",
            "\tEpoch 004/005: Loss at step 50: 0.042547457 2019-05-08 14:47:07.788030\n",
            "\t               WholeLoss: 0.23541 DevLoss: 0.24854\n",
            "\tEpoch 004/005: Loss at step 60: 0.038687240 2019-05-08 14:47:22.277020\n",
            "\t               WholeLoss: 0.28798 DevLoss: 0.25069\n",
            "\tEpoch 004/005: Loss at step 70: 0.038299888 2019-05-08 14:47:36.756123\n",
            "\t               WholeLoss: 0.24902 DevLoss: 0.2457\n",
            "\tEpoch 004/005: Loss at step 80: 0.040503792 2019-05-08 14:47:51.113357\n",
            "\t               WholeLoss: 0.23566 DevLoss: 0.25492\n",
            "\tEpoch 004/005: Loss at step 90: 0.039816849 2019-05-08 14:48:05.481622\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.23189 DevLoss: 0.23418\n",
            "\tEpoch 004/005: Loss at step 100: 0.036831979 2019-05-08 14:48:19.897829\n",
            "\t               WholeLoss: 0.23472 DevLoss: 0.23574\n",
            "\tEpoch 004/005: Loss at step 110: 0.037901409 2019-05-08 14:48:34.227247\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.22272 DevLoss: 0.22987\n",
            "\tEpoch 004/005: Loss at step 120: 0.038972467 2019-05-08 14:48:48.583348\n",
            "\t               WholeLoss: 0.25833 DevLoss: 0.25365\n",
            "\tEpoch 004/005: Loss at step 130: 0.038173184 2019-05-08 14:49:02.966669\n",
            "\t               WholeLoss: 0.26344 DevLoss: 0.25075\n",
            "\tEpoch 004/005: Loss at step 140: 0.037576243 2019-05-08 14:49:17.484021\n",
            "\t               WholeLoss: 0.21798 DevLoss: 0.2316\n",
            "\tEpoch 004/005: Loss at step 150: 0.037421510 2019-05-08 14:49:31.912703\n",
            "\t               WholeLoss: 0.27066 DevLoss: 0.26148\n",
            "\tEpoch 004/005: Loss at step 160: 0.038018476 2019-05-08 14:49:46.295577\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.20974 DevLoss: 0.22457\n",
            "\tEpoch 004/005: Loss at step 170: 0.040983796 2019-05-08 14:50:00.832796\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.22581 DevLoss: 0.22388\n",
            "\tEpoch 004/005: Loss at step 180: 0.039634552 2019-05-08 14:50:16.123908\n",
            "\t               WholeLoss: 0.20243 DevLoss: 0.22941\n",
            "\tEpoch 004/005: Loss at step 190: 0.038047098 2019-05-08 14:50:30.530987\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.25856 DevLoss: 0.2217\n",
            "Epoch 004/005 completed \t - \tBatch loss: 0.039111368 2019-05-08 14:50:43.768028\n",
            "NEW EPOCH: 5\n",
            "\tEpoch 005/005: Loss at step 00: 0.039894037 2019-05-08 14:50:44.856082\n",
            "\t               WholeLoss: 0.27482 DevLoss: 0.26932\n",
            "\tEpoch 005/005: Loss at step 10: 0.039043948 2019-05-08 14:50:59.277523\n",
            "\t               WholeLoss: 0.23838 DevLoss: 0.22599\n",
            "\tEpoch 005/005: Loss at step 20: 0.042322136 2019-05-08 14:51:13.692643\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.22375 DevLoss: 0.21389\n",
            "\tEpoch 005/005: Loss at step 30: 0.041171044 2019-05-08 14:51:28.092866\n",
            "\t               WholeLoss: 0.23899 DevLoss: 0.24611\n",
            "\tEpoch 005/005: Loss at step 40: 0.037832808 2019-05-08 14:51:42.458824\n",
            "\t               WholeLoss: 0.21996 DevLoss: 0.2186\n",
            "\tEpoch 005/005: Loss at step 50: 0.043597102 2019-05-08 14:51:57.082681\n",
            "\t               WholeLoss: 0.20649 DevLoss: 0.21776\n",
            "\tEpoch 005/005: Loss at step 60: 0.038293555 2019-05-08 14:52:12.944272\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.21072 DevLoss: 0.21\n",
            "\tEpoch 005/005: Loss at step 70: 0.038876526 2019-05-08 14:52:27.318390\n",
            "\t               WholeLoss: 0.22579 DevLoss: 0.23172\n",
            "\tEpoch 005/005: Loss at step 80: 0.039350629 2019-05-08 14:52:41.679261\n",
            "\t               WholeLoss: 0.21641 DevLoss: 0.22083\n",
            "\tEpoch 005/005: Loss at step 90: 0.041883454 2019-05-08 14:52:56.096760\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.21355 DevLoss: 0.20917\n",
            "\tEpoch 005/005: Loss at step 100: 0.037652858 2019-05-08 14:53:10.492756\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.19701 DevLoss: 0.19749\n",
            "\tEpoch 005/005: Loss at step 110: 0.039075010 2019-05-08 14:53:24.902685\n",
            "\t               WholeLoss: 0.19427 DevLoss: 0.19984\n",
            "\tEpoch 005/005: Loss at step 120: 0.039684035 2019-05-08 14:53:39.397642\n",
            "\t               WholeLoss: 0.23432 DevLoss: 0.23699\n",
            "\tEpoch 005/005: Loss at step 130: 0.039017856 2019-05-08 14:53:53.799222\n",
            "\t               WholeLoss: 0.20964 DevLoss: 0.2124\n",
            "\tEpoch 005/005: Loss at step 140: 0.036171272 2019-05-08 14:54:08.163108\n",
            "\t               WholeLoss: 0.20632 DevLoss: 0.24205\n",
            "\tEpoch 005/005: Loss at step 150: 0.037211251 2019-05-08 14:54:22.479591\n",
            " ----------------------------------------------------------------- new best found ^.^\n",
            "\t               WholeLoss: 0.21191 DevLoss: 0.19174\n",
            "\tEpoch 005/005: Loss at step 160: 0.036483325 2019-05-08 14:54:36.898261\n",
            "\t               WholeLoss: 0.1795 DevLoss: 0.19774\n",
            "\tEpoch 005/005: Loss at step 170: 0.038758975 2019-05-08 14:54:51.154051\n",
            "\t               WholeLoss: 0.20357 DevLoss: 0.21135\n",
            "\tEpoch 005/005: Loss at step 180: 0.038300537 2019-05-08 14:55:05.614198\n",
            "\t               WholeLoss: 0.19087 DevLoss: 0.20964\n",
            "\tEpoch 005/005: Loss at step 190: 0.038530797 2019-05-08 14:55:20.732146\n",
            "\t               WholeLoss: 0.23926 DevLoss: 0.20544\n",
            "Epoch 005/005 completed \t - \tBatch loss: 0.038777292 2019-05-08 14:55:34.039850\n",
            "Final loss: 0.038777292 2019-05-08 14:55:34.042214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0GCUvlzuXxya"
      },
      "source": [
        "#### Loss plot over the entire training sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rucOwPRmG0fv",
        "colab_type": "code",
        "outputId": "1ef4a9df-51ac-418a-d9ce-305ae8f0644a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print('best train loss',bestwholeloss.numpy())\n",
        "print('best dev loss',bestdevloss)\n",
        "print('best epoch',bestepoch)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best train loss 0.21191141\n",
            "best dev loss 0.19173814306656517\n",
            "best epoch 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-8xq0E1V_57",
        "colab_type": "code",
        "outputId": "e84e5342-64c4-4b9f-adfa-d71772982b56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.title(\"Loss vs. Iterations\")\n",
        "plt.ylabel('Cross Entropy Loss')\n",
        "plt.xlabel('Batch')\n",
        "plt.plot(loss_history,label='Sequence Batch Train Loss',color='grey',alpha=0.3)\n",
        "plt.plot(wholeLoss_history,label='Avg. Train Loss',color='blue',alpha=0.5)\n",
        "plt.plot(devLoss_history,label='Dev Loss',color='orange')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAJcCAYAAABXOLh8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmYiIiEjClJCJiIiIJOz/ARVOM6O3Y0JQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D8wqL9Rrp1O",
        "colab_type": "text"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj8KZRWIrp1O",
        "colab_type": "code",
        "outputId": "83baccfd-7eb4-4f77-90b3-25fbaa7c8069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# print loss summary\n",
        "aloss1=0\n",
        "interloss=0\n",
        "# Use data point\n",
        "for seqLen in range(minSeqSize,maxSeqSize+1):\n",
        "  X,Y=makeData(seqLen,seqLen,testBatchSize,testNumBatches)\n",
        "  interloss=infer(encoder,decoder,attention,X[seqLen],Y[seqLen])\n",
        "  print('Seq:',seqLen,'Size:',X[seqLen].shape[0],'loss:',interloss)\n",
        "  aloss1+=interloss/(maxSeqSize+1-minSeqSize)\n",
        "\n",
        "print('---')\n",
        "aloss2=0\n",
        "interloss=0\n",
        "# Use data point\n",
        "for seqLen in range(maxSeqSize+1,maxSeqSize+6):\n",
        "  X,Y=makeData(seqLen,seqLen,testBatchSize,testNumBatches)\n",
        "  interloss=infer(encoder,decoder,attention,X[seqLen],Y[seqLen])\n",
        "  print('Seq:',seqLen,'Size:',X[seqLen].shape[0],'loss:',interloss)\n",
        "  aloss2+=interloss/(5)\n",
        "print()\n",
        "print('aloss1:',np.round(aloss1,6))\n",
        "print('aloss2:',np.round(aloss2,6))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seq: 2 Size: 640 loss: 0.006781505420804024\n",
            "Seq: 3 Size: 640 loss: 0.09650561213493347\n",
            "Seq: 4 Size: 640 loss: 0.24197398126125336\n",
            "Seq: 5 Size: 640 loss: 0.47921147346496584\n",
            "---\n",
            "Seq: 6 Size: 640 loss: 1.0093003908793132\n",
            "Seq: 7 Size: 640 loss: 1.6724741799490792\n",
            "Seq: 8 Size: 640 loss: 1.9665846824645996\n",
            "Seq: 9 Size: 640 loss: 2.209872775607639\n",
            "Seq: 10 Size: 640 loss: 2.397024154663086\n",
            "\n",
            "aloss1: 0.206118\n",
            "aloss2: 1.851051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RhufYt4_c7V",
        "colab_type": "code",
        "outputId": "b349d127-81ed-4111-8c08-44b09d9b5b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6596
        }
      },
      "source": [
        "#Print verbose level 1 detailed examples\n",
        "for seqLen in range(minSeqSize,maxSeqSize+6):\n",
        "  X,Y=makeData(seqLen,seqLen,2,1)\n",
        "  print('Seq:',seqLen)\n",
        "  print('\\tloss:',infer(encoder,decoder,attention,X[seqLen],Y[seqLen],1))\n",
        "  print('\\n\\n==============\\n\\n')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seq: 2\n",
            "\n",
            "===== PREDICTED: 0 =====\n",
            "\n",
            "\t- [0.         0.70881593]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 0 ---\n",
            "\n",
            "\t- [0.         0.70881593]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 1 =====\n",
            "\n",
            "\t- [0.         0.09216173]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 1 ---\n",
            "\n",
            "\t- [0.         0.09216173]\n",
            "\t- [1. 0.]\n",
            "\tloss: 0.00706768361851573\n",
            "\n",
            "\n",
            "==============\n",
            "\n",
            "\n",
            "Seq: 3\n",
            "\n",
            "===== PREDICTED: 0 =====\n",
            "\n",
            "\t- [0.         0.37492886]\n",
            "\t- [0.         0.63655484]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 0 ---\n",
            "\n",
            "\t- [0.         0.37492886]\n",
            "\t- [0.         0.63655484]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 1 =====\n",
            "\n",
            "\t- [0.         0.13522436]\n",
            "\t- [0.         0.91253203]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 1 ---\n",
            "\n",
            "\t- [0.         0.13522436]\n",
            "\t- [0.         0.91253203]\n",
            "\t- [1. 0.]\n",
            "\tloss: 0.02731916805108388\n",
            "\n",
            "\n",
            "==============\n",
            "\n",
            "\n",
            "Seq: 4\n",
            "\n",
            "===== PREDICTED: 0 =====\n",
            "\n",
            "\t- [0.         0.45698878]\n",
            "\t- [0.         0.49762738]\n",
            "\t- [0.        0.5424042]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 0 ---\n",
            "\n",
            "\t- [0.         0.45698878]\n",
            "\t- [0.         0.49762738]\n",
            "\t- [0.        0.5424042]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 1 =====\n",
            "\n",
            "\t- [0.         0.19463399]\n",
            "\t- [0.        0.6254916]\n",
            "\t- [0.        0.6225439]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 1 ---\n",
            "\n",
            "\t- [0.         0.19463399]\n",
            "\t- [0.        0.6225439]\n",
            "\t- [0.        0.6254916]\n",
            "\t- [1. 0.]\n",
            "\tloss: 0.5865538120269775\n",
            "\n",
            "\n",
            "==============\n",
            "\n",
            "\n",
            "Seq: 5\n",
            "\n",
            "===== PREDICTED: 0 =====\n",
            "\n",
            "\t- [0.         0.00748427]\n",
            "\t- [0.         0.35245094]\n",
            "\t- [0.         0.71474206]\n",
            "\t- [0.        0.7718665]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 0 ---\n",
            "\n",
            "\t- [0.         0.00748427]\n",
            "\t- [0.         0.35245094]\n",
            "\t- [0.         0.71474206]\n",
            "\t- [0.        0.7718665]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 1 =====\n",
            "\n",
            "\t- [0.         0.35250437]\n",
            "\t- [0.        0.3037139]\n",
            "\t- [0.        0.3728977]\n",
            "\t- [0.      0.71634]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 1 ---\n",
            "\n",
            "\t- [0.        0.3037139]\n",
            "\t- [0.         0.35250437]\n",
            "\t- [0.        0.3728977]\n",
            "\t- [0.      0.71634]\n",
            "\t- [1. 0.]\n",
            "\tloss: 0.47209830284118653\n",
            "\n",
            "\n",
            "==============\n",
            "\n",
            "\n",
            "Seq: 6\n",
            "\n",
            "===== PREDICTED: 0 =====\n",
            "\n",
            "\t- [0.         0.11454606]\n",
            "\t- [0.         0.11454606]\n",
            "\t- [0.         0.69825053]\n",
            "\t- [0.         0.57563835]\n",
            "\t- [0.         0.69825053]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 0 ---\n",
            "\n",
            "\t- [0.         0.11454606]\n",
            "\t- [0.         0.15983787]\n",
            "\t- [0.        0.4451671]\n",
            "\t- [0.         0.57563835]\n",
            "\t- [0.         0.69825053]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 1 =====\n",
            "\n",
            "\t- [0.         0.34628978]\n",
            "\t- [0.         0.50937945]\n",
            "\t- [0.         0.69722867]\n",
            "\t- [0.         0.89963377]\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 1 ---\n",
            "\n",
            "\t- [0.         0.34628978]\n",
            "\t- [0.         0.50937945]\n",
            "\t- [0.         0.57409656]\n",
            "\t- [0.         0.69722867]\n",
            "\t- [0.         0.89963377]\n",
            "\t- [1. 0.]\n",
            "\tloss: 1.0192336241404216\n",
            "\n",
            "\n",
            "==============\n",
            "\n",
            "\n",
            "Seq: 7\n",
            "\n",
            "===== PREDICTED: 0 =====\n",
            "\n",
            "\t- [0.         0.05334431]\n",
            "\t- [0.         0.05334431]\n",
            "\t- [0.        0.6613214]\n",
            "\t- [0.         0.66830087]\n",
            "\t- [0.         0.85078746]\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 0 ---\n",
            "\n",
            "\t- [0.         0.05334431]\n",
            "\t- [0.         0.23284894]\n",
            "\t- [0.         0.50162965]\n",
            "\t- [0.        0.6613214]\n",
            "\t- [0.         0.66830087]\n",
            "\t- [0.         0.85078746]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 1 =====\n",
            "\n",
            "\t- [0.         0.36954224]\n",
            "\t- [0.        0.5122663]\n",
            "\t- [0.        0.8612174]\n",
            "\t- [0.        0.7866113]\n",
            "\t- [0.        0.7866113]\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 1 ---\n",
            "\n",
            "\t- [0.         0.36954224]\n",
            "\t- [0.        0.5122663]\n",
            "\t- [0.        0.6119537]\n",
            "\t- [0.        0.7866113]\n",
            "\t- [0.        0.8612174]\n",
            "\t- [0.        0.9150203]\n",
            "\t- [1. 0.]\n",
            "\tloss: 1.7862858091081892\n",
            "\n",
            "\n",
            "==============\n",
            "\n",
            "\n",
            "Seq: 8\n",
            "\n",
            "===== PREDICTED: 0 =====\n",
            "\n",
            "\t- [0.         0.00956633]\n",
            "\t- [0.         0.29138246]\n",
            "\t- [0.        0.8836367]\n",
            "\t- [0.        0.8836367]\n",
            "\t- [0.        0.9658303]\n",
            "\t- [0.        0.9658303]\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 0 ---\n",
            "\n",
            "\t- [0.         0.00956633]\n",
            "\t- [0.        0.2070446]\n",
            "\t- [0.         0.29138246]\n",
            "\t- [0.         0.65382016]\n",
            "\t- [0.         0.83520204]\n",
            "\t- [0.        0.8836367]\n",
            "\t- [0.        0.9658303]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 1 =====\n",
            "\n",
            "\t- [0.         0.16783535]\n",
            "\t- [0.         0.29259762]\n",
            "\t- [0.         0.29259762]\n",
            "\t- [0.        0.7556874]\n",
            "\t- [0.        0.7556874]\n",
            "\t- [0.        0.7556874]\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 1 ---\n",
            "\n",
            "\t- [0.         0.16783535]\n",
            "\t- [0.         0.19183518]\n",
            "\t- [0.         0.29259762]\n",
            "\t- [0.         0.49148154]\n",
            "\t- [0.        0.7556874]\n",
            "\t- [0.        0.8066029]\n",
            "\t- [0.        0.8386172]\n",
            "\t- [1. 0.]\n",
            "\tloss: 1.7588053941726685\n",
            "\n",
            "\n",
            "==============\n",
            "\n",
            "\n",
            "Seq: 9\n",
            "\n",
            "===== PREDICTED: 0 =====\n",
            "\n",
            "\t- [0.         0.00847956]\n",
            "\t- [0.         0.05145267]\n",
            "\t- [0.         0.46845508]\n",
            "\t- [0.         0.44841573]\n",
            "\t- [0.         0.90671486]\n",
            "\t- [0.         0.84784204]\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 0 ---\n",
            "\n",
            "\t- [0.         0.00847956]\n",
            "\t- [0.         0.05145267]\n",
            "\t- [0.        0.1752823]\n",
            "\t- [0.         0.44841573]\n",
            "\t- [0.         0.46845508]\n",
            "\t- [0.        0.5767189]\n",
            "\t- [0.         0.84784204]\n",
            "\t- [0.         0.90671486]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 1 =====\n",
            "\n",
            "\t- [0.         0.44417745]\n",
            "\t- [0.         0.44417745]\n",
            "\t- [0.         0.23768188]\n",
            "\t- [0.         0.56562555]\n",
            "\t- [0.        0.8170687]\n",
            "\t- [0.        0.8170687]\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 1 ---\n",
            "\n",
            "\t- [0.         0.23768188]\n",
            "\t- [0.         0.40443963]\n",
            "\t- [0.         0.44417745]\n",
            "\t- [0.        0.4739306]\n",
            "\t- [0.         0.56562555]\n",
            "\t- [0.        0.8043743]\n",
            "\t- [0.        0.8170687]\n",
            "\t- [0.         0.83048505]\n",
            "\t- [1. 0.]\n",
            "\tloss: 2.1362082163492837\n",
            "\n",
            "\n",
            "==============\n",
            "\n",
            "\n",
            "Seq: 10\n",
            "\n",
            "===== PREDICTED: 0 =====\n",
            "\n",
            "\t- [0.         0.10792513]\n",
            "\t- [0.         0.85125357]\n",
            "\t- [0.      0.94157]\n",
            "\t- [0.       0.877711]\n",
            "\t- [0.       0.877711]\n",
            "\t- [0.        0.8454665]\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 0 ---\n",
            "\n",
            "\t- [0.         0.10792513]\n",
            "\t- [0.         0.37740532]\n",
            "\t- [0.        0.6913558]\n",
            "\t- [0.        0.8454665]\n",
            "\t- [0.         0.85125357]\n",
            "\t- [0.       0.877711]\n",
            "\t- [0.        0.9267079]\n",
            "\t- [0.      0.94157]\n",
            "\t- [0.         0.95658654]\n",
            "\t- [1. 0.]\n",
            "\n",
            "===== PREDICTED: 1 =====\n",
            "\n",
            "\t- [0.         0.09133491]\n",
            "\t- [0.         0.09177175]\n",
            "\t- [0.        0.7935717]\n",
            "\t- [0.         0.96103764]\n",
            "\t- [0.         0.96103764]\n",
            "\t- [0.         0.97400826]\n",
            "\t- [0.         0.97400826]\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\t- [1. 0.]\n",
            "\n",
            "--- ACTUAL: 1 ---\n",
            "\n",
            "\t- [0.         0.09133491]\n",
            "\t- [0.         0.09177175]\n",
            "\t- [0.         0.35452038]\n",
            "\t- [0.        0.5481831]\n",
            "\t- [0.        0.7935717]\n",
            "\t- [0.        0.8064308]\n",
            "\t- [0.         0.90273964]\n",
            "\t- [0.         0.96103764]\n",
            "\t- [0.         0.97400826]\n",
            "\t- [1. 0.]\n",
            "\tloss: 2.462627410888672\n",
            "\n",
            "\n",
            "==============\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}